## 현재 작업 리스트
1. octave 코드를 python 코드로 변환중
2. 텐서플로우2로 코드 변환중

## 머신러닝을 이해하는 거대관점 정리중
1. 풀이 방법: 정보의 추상화
    - 확률대수관점 MLE
    - 기하관점 벡터공간 변환
      
2. 지도 학습: 데이터 분류 trained by x and y > x를 넣으면 확률 y를 알려주는 확률분포를 얻는다
    - 고전: svm hmm naive bayes regression
    - 현대: dnn cnn(비시계열 데이터) rnn trans bert gpt(시계열 데이터)
    - 응용: yolo resnet style-transfer concentrate

3. 비지도학습: 데이터 분류 trained by x > x를 넣으면 확률 y를 알려주는 확률분포를 얻는다
    - 이진분류: gaussian abnormaly > svm: 1개의 확률분포를 얻는다
    - 다중분류: k-means > gm > em > vi: 여러개의 확률분포를 얻는다
    
4. 지도 + 비지도: 데이터 생성 by latent z
    - find latent: pca kpca t-sne encoder > x를 넣으면 latent z를 알려주는 확률분포를 얻는다
    - create data: decoder > 데이터를 높은 확률로 샘플링하는 확률분포를 얻는다
    - 차원축소 이진분류 단일생성: encoder + decoder = ae
    - 차원축소 다중분류 + 다중생성: vae > gan

5. 강화학습: 셀프 판단 or 행동
    - 정책기반: 폴리시이터 q러닝 reinforce
    - 가치기반: 밸류이터 sarsa
    - 가치정책기반: deepsarsa dqn a2c

6. 텐서플로우와 케라스를 활용해서 핵심 알고리즘을 구현하고 현실의 문제를 해결

## 지도학습 알고리즘
머신러닝1 : 선형 회귀 : 데이터의 "진행"패턴이 존재하는 상황에서 다음 데이터의 값을 예측하는 함수를 만들고 싶다

피쳐 스케일링
가설함수 형태 : 무리, 선형, 다항함수로 적당히 조합해서 가정
차이값 제곱의 합 / 2m =  코스트 함수(오버피팅도 동시에 해결하자)
코스트 함수가 최소가 되는 세타를 구한다
-코스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
-노말 이퀘이젼
-코스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
세타로 함수확정 = 결과의 분포와 비슷한 함수



머신러닝2 : 로지스틱 회귀 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
(매우 단순한 분류만 가능하다)(데이터의 분포패턴 = 벡터)

피쳐 스케일링
가설함수 형태 : 무리, 선형, 다항함수로 적당히 조합해서 가정
로그와 시그모이드(가설함수값)을 잘 버무려서 / m = 코스트 함수(오버피팅도 동시에 해결하자)
코스트 함수가 최소가 되는 세타를 구한다
-코스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
-코스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
세타로 함수확정 = 분류를 결정하는 디시젼 바운더리



머신러닝3 : 뉴럴 네트워크 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
(변수끼리 관계에 의한 매우 복잡한 분류도 가능하다)(데이터의 분포패턴 = 벡터)
(아웃풋 레이어의 개수에 따라 멀티분류가 자체적으로 가능하다)

로지스틱 회귀를 사용한 단일 레이어로는 분류가 불가능한 데이터 패턴이 존재한다
로지스틱 회귀의 분류값을 뉴런의 활성유뮤로 생각하자
로지스틱 회귀를 사용한 뉴런의 다중 레이어를 이용하면 단일 레이어로 분류가 불가능한 데이터 패턴도 분류할 수 있다
최적화 함수를 잘 쓰자(옵션 정의 / 이니셜 세타 정의 / 코스트펑션에 코스트함수와 미분계수 정의 / 최적화세타 = fminunc(코스트펑션/이니셜세타/옵션))



머신러닝4 : 실습 : 스팸 분류기를 만들어 보자

이메일 데이터를 활용해서 이메일에 자주 등장하는 단어사전을 생성한다
각각의 이메일 단어 분포패턴 기록 >> 단어의 등장유무에 따라 0 / 1을 부여해서 각각의 이메일을 벡터화
 : 이메일을 벡터화할 때, 단어의 분포패턴 이외에 이메일의 헤더 분포패턴도 활용하면 좋다
 : 단어사전을 만들 때, 원형이 같은 단어는 같은 단어로 분류하는 것이 효율성 측면에서 좋다
 : 단어사전을 만들 때, 단어의 디테일한 측면(이상한 단어의 등장, 이상한 기호의 등장)을 잘 반영하면 분포패턴이 더 정확하다
뉴럴네트워크로 분류한다



머신러닝5 : 실습 : 암 분류기를 만들어 보자

말짱하다 99% / 암이다 1% 의 데이터가 있다고 하자
 : 모든 사람이 말짱하다 = 99% 맞았다
 : 머신러닝 열심히 = 99% 맞았다
 : 이렇게 되면 머신러닝이 의미가 없다

프리시젼 = 암으로 분류가 진짜였다 / 암으로 분류
리콜 = 암으로 분류가 진짜였다 / 원래 암

모든 사람이 말짱하다
 : 프리시젼 = 값이 정의되지 않는다(모두 말짱하다고 했으므로 암으로 분류한 적이 없다)
 : 리콜 = 0(극소수이지만 원래 암인 사람을 / 암이라고 맞춘 적이 없다)
 : 이 알고리즘은 구라임을 알 수 있더
 : 따라서 프리시젼과 리콜은 데이터의 1%에 해당하는 분류결과로 정의

# 시그모이드(가설함수) = 확률 : 확률이 얼마 이상이면 y = 1이라고 예측하는 것이 좋을까?

예측확률 > 0.9 : 1이라고 생각한다 = 예측한 1이 / 실제 1이었을 확률이 높다 = 프리시젼이 높다 
예측확률 < 0.9 : 0이라고 생각한다 = 실제 1인데 / 1이라고 예측할 확률이 낮다 = 리콜이 낮다

예측확률 > 0.1 : 1이라고 생각한다 = 예측한 1이 / 실제 1이었을 확률이 낮다 = 프리시젼이 낮다 
예측확률 < 0.1 : 0이라고 생각한다 = 실제 1인데 / 1이라고 예측할 확률이 높다 = 리콜이 높다

그렇다면 기준값을 어떻게 정할까?
프리시젼과 리콜의 비율로 결정한다
즉 (0 < 프리시젼과 리콜의 조화평균 < 1)을 가장 크게 만드는 값을 선택한다

# a / b / c 멀티분류
 : a에 대해 분류해서 얻은 확률함수값 / b에대해 분류해서 얻은 확률함수값 / 1 - (앞의 두 확률의 합)
 : 새로운 데이터가 주어지면 세 값을 모두 계산하고 확률함수값이 max인 클래스를 선택한다

# 이니셜 세타
 : 0이 나오면 망하는 경우가 많으니까 0-1사이의 랜덤값으로 이루어진 행렬을 만들고
 : 곱하기 2엡실론 마이너스 엡실론
 : 범위를 -엡 +엡 사이로 만든다
 
# 내 그래디언트 디센트가 맞는지 틀렸는지 불안하면 그래디언트 체킹을 하자
 : 매우 작은 입실론값에 대해 평균변화율을 계산해서 미분계수와 거의 일치하면 맞는것으로 인정
 : 백프로파게이션을 한 번정도 시행하고 일치여부를 확인하며, 거의 일치하면 체킹을 끄고 본격적으로 전파왕복을 시작해야 한다

# 오버피팅이란?
 : 트레이닝셋으로는 잘 맞았는데, 트레이닝 셋에만 너무나도 잘 맞는 가설함수를 선택한 나머지, 실시간 데이터에 대해 예측이 심하게 빗나가는 현상을 말한다

# 오버피팅 해결1
 : 피쳐의 개수와 식의 복잡도를 줄이자

# 오버피팅 해결2
 : 선형 회귀 / 로지스틱 회귀 / 뉴럴 네트워크 모두 코스트 함수에 (바이어스 세타를 제외한 모든 세타의 제곱의 합 * 람다 / 2m)을 더한다
 : 미분하면 (바이어스 세타를 제외한 모든 세타 * 람다 / m)이다
 : 적당히 큰 람다라면 코스트 함수가 최소값을 가지기 위해 자동으로 세타를 작게 만들게 된다 : 그 세타에 붙어있는 피쳐의 영향력이 줄어든다 : 가설함수의 차수가 낮아지는 효과가 생긴다 : 오버피팅 해결!



# 오버피팅 해결3
 : 노말이퀘이젼 괄호안에 (람다 * 단위행렬(바이어스 부분은 0))을 더하고 계산한다



# 진단1 : 내가 만든 머신러닝 프로그램의 가설함수가 오버피팅을 유발하는 함수인지? 상태를 진단하고 올바른 가설함수 형태 찾아보기! (차수와 람다를 조절해서 피쳐의 개수에 영향을 줘보자!)
 : 트레이닝셋 60% / 평가셋 20% / 테스트셋 20%

 : 10개의 람다를 만들고 각각의 람다에 대해 10개의 다양한 가설함수를 만들어서(100개를 만들어라ㄷㄷ)
 
 : 첫번째 람다에 대한 10개의 가설함수로 트레이닝셋에 적용해서 각각의 경우에 대해 코스트함수가 최소가 되는 세타 10개를 구한다(일단 트레이닝)
  : (이 때는 너무 당연하게 코스트 함수에 람다를 쓴다!)
  : (트레이닝셋에만 최적화된 세타들ㅠ)
  : (당연히 가설함수의 차수가 높아짐에 따라 코스트 함수가 작아진다)

 : 10개의 세타를 평가셋에 적용해서 코스트 함수가 최소가 되는 세타와 가설함수를 고른다(이제 평가)
  : (이 때는 같은 람다로 만든 세타들 중에서 고르는 거니까, 너무 당연하게 코스트 함수에 람다를 쓰지 않는다!)
  : (이렇게 해서 트레이닝셋에만 최적화되지 않는, 일반적으로 코스트 함수를 최소화하는 세타를 찾을 수 있다!)
  : (당연히 가설함수의 차수가 낮은 상태에서 높은 상태로 변함에 따라 코스트 함수는 감소하다가 커진다 = 언더피팅에서 최적화상태가 되었다가 오버피팅된다)

 : 위의 과정을 간단히 정리하면 다음과 같다
 : 트레이닝셋코스트와 평가셋코스트가 모두 높고 비슷하면 = 언더피팅 / 트레이닝셋코스트는 작은데 평가셋코스트만 높으면 = 오버피팅

 : 이렇게 100개를 반복해서 최강의 콤보(람다와 차수)를 결정한다

 : 최종 콤보를 테스트셋에 적용해서 코스트 함수를 최소화해보고, 그래도 문제가 있다면 위의 작업을 다시 해보자



# 평가셋과 테스트셋의 코스트함수 특이하게 사용하기
 : 로지스틱 회귀는 테스트셋으로 코스트 함수를 계산할 때
  : 잘못된 결과(가설함수는 0보다 큰데 y는 0이라던지 / 가설함수는 0보다 작은데 y는 1이라던지) = 1
  : 옳은 결과 = 0
  : 위의 두 결과를 다 더해서 m으로 나누는 방법으로도 계산할 수 있다



# 진단2 : 내가 만든 머신러닝 프로그램의 가설함수가 오버피팅을 유발하는 함수인지? 상태를 진단하고 올바른 가설함수 형태 찾아보기! (데이터를 추가하는 것이 도움이 될까?)
 : 트레이닝셋의 개수를 1 > 100개로 증가시키면서 트레이닝셋의 코스트 함수를 계산 = 모든 데이터를 다 맞추다가 오차가 발생하기 시작 = 코스트 함수값은 0 > 50
 : 트레이닝셋의 개수를 1 > 100개로 증가시키면서 평가셋 전체의 코스트 함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하면서 오차가 감소 = 코스트함수값은 100 > 50

 : 언더피팅 상태라면
  : 트레이닝셋의 코스트함수를 계산 = 모든 데이터를 다 맞추다가 오차가 크게 발생하는데, 큰 오차를 줄이는데 한계가 빠르게 발생 = 코스트 함수값은 0 > 70
  : 평가셋의 코스트함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하기는 하는데, 성능이 증가하는데 한계가 빠르게 발생 = 코스트 함수값은 100 > 70
  : 따라서 이미 언더피팅인 상태에서는, 한계가 빠르게 발생하기 때문에, 데이터를 더 입력해도 의미가 없다
  : 진단 1의 방법으로 차수와 람다를 조절해야 한다

 : 오버피팅 상태라면
  : 트레이닝셋의 코스트함수를 계산 = 모든 데이터를 다 맞추다가 오차가 사알짝 발생하지만, 그 오차가 거의 없다 = 코스트 함수값은 0 > 10
  : 평가셋의 코스트함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하기는 하는데, 트레이닝셋만 너무 잘맞추고 평가셋에 대해서는 일반화가 안됨 = 코스트 함수값은 100 > 80
  : 따라서 이미 오버피팅인 상태에서는, 위의 두 값의 차이가 크기 때문에, 차이가 감소할때까지 데이터를 더 입력하는 것이 크게 도움이 된다
  : 진단 1의 방법으로 차수와 람다를 조절해야 한다 + 데이터를 더 투입한다



# 진단 후 해결방법 총정리
 : 오버피팅이라면? : 차수 감소 / 람다 증가 / 더 많은 트레이닝셋 투입(많은 데이터는 오버피팅을 상쇄한다) / 뉴럴 네트워크라면 레이어와 파라미터 개수 감소(람다를 활용하는게 더 낫더라)
 : 언더피팅이라면? : 차수 증가 / 람다 감소 / 더 많은 트레이닝셋 투입은 무의미 / 뉴럴 네트워크라면 레이어와 파라미터 개수 증가(람다를 활용하는게 더 낫더라)
 
 
 
 머신러닝6 : 서포트 벡터 머신 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
(변수끼리 관계에 의한 매우 복잡한 분류도 가능하다)(데이터의 분포패턴 = 벡터)

피쳐 스케일링?
가설함수 형태 : 커널함수를 사용해서 변수를 트레이닝셋의 개수만큼 생성한다
relu와 시그모이드(가설함수값)을 잘 버무려서 = 코스트 함수(오버피팅도 동시에 해결하자)(람다 대신 c를 쓴다)
코스트 함수가 최소가 되는 세타를 구한다
-코스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
-코스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
세타로 함수확정 = 분류를 결정하는 디시젼 바운더리



# 확률과 추정에 관한 정리1(로지스틱 회귀 + 뉴럴 네트워크)
 : 데이터의 y = 1이라면
 : 시그모이드(z) = h(z) = 1에 가까워야 한다
 : cost(h(z)) = h(z)가 1에 가까울수록 작아지게 만들고 + cost(h(z))를 최소화하는 z를 찾는다(실제 y와 h(z)의 오차가 0에 가까워 지는 순간까지 z를 찾는다)
 : 찾은 z = 디시젼 바운더리

 : 이제 새로운 데이터가 들어오면 디시젼 바운더리 z를 활용해서
 : z를 경계로 z > 0 이면 / 시그모이드(z) = h(z) > 0.5 이면 / y = 1로 판단한다



# 커널함수로 변수 생성하기
 : 트레이닝셋 1000개 중에서 1번 데이터
 : 데이터의 y = 1이라면
 : 데이터의 변수 x1, x2 >> 커널함수에 넣으면 >> 트레이닝셋의 개수인 1000개만큼의 변수 x1 - x1000 생성
 : 각 변수는 현재 데이터와의 비슷한 정도를 0 - 1 사이의 숫자로 나타낸다
 : 비슷할수록 1 / 다를수록 0이다
 : 비슷하면 값을 증가시키고, 다르면 값을 감소시켜서 모두 더한 합을 z라 한다
 : h(z)가 1에 가까워야 한다



# 확률과 추정에 관한 정리2(서포트 벡터 머신)
 : 데이터의 y = 1이라면
 : 시그모이드(z) = h(z) = 1에 가까워야 한다
 : cost(h(z)) = h(z)가 0.8에 가까울수록 작아지게 만들고 + 0.8 이상이면 0이 되도록 만들고 + cost(h(z))를 최소화하는 z를 찾는다(실제 y와 h(z)의 오차가 0.2에 가까워 지는 순간까지 z를 찾는다)
 : 찾은 z = 디시젼바운더리

 : 이제 새로운 데이터가 들어오면 디시젼 바운더리 z를 활용해서
 : z를 경계로 z > 0이면 / 시그모이드(z) = h(z) > 0.5 이면 / y = 1로 판단한다



# 서포트 벡터 머신의 코스트함수 최소화 과정
 : cost(h(z)) 최소화

 : cost(h(z)) = 코스트함수부분 + 오버피팅부분
  : 코스트함수부분 최소화 = 0으로 최소화
  : h(z) > 0.8이 되도록 z를 조절한다
  : z > 1이 되도록 z가 조절한다
  : z = 세타와 x의 내적이다
  : z = 세타길이 * x길이 * 코싸인 > 1이 되도록 z가 조절한다(x길이는 일정하다)
 : 오버피팅부분 최소화
  : 세타길이 최소화

 : 위의 두 결론을 종합하면, 코스트함수부분을 0으로 최소화하면서, 오버피팅부분 최소화하려면, 코싸인값이 커져야 한다 = 코싸인이 1이다 = 각이 0 = 세타와 x는 거의 같은 방향의 벡터다
 : 세타는 디시젼 바운더리 z의 법선벡터이기 때문에, 이 상황을 그림으로 생각해 보면, 법선벡터가 데이터를 가르키는 벡터와 거의 평행하다는 뜻이다
 : 디시젼 바운더리는 자연스럽게 두 데이터 집단의 가운데에 마진을 가지고 존재하게 된다
 
 
 
 
1. regression
    - linear
    - logistic
2. dnn
3. cnn
    - cnn
        - 프로젝트: face recognition
    - yolo
    - neural style transfer
    - resnet
    - u net

4. rnn
    - rnn
    - lstm
        - composition
        - emojify
        - writing
        - trigger
        - translation
    - transformer
    - bert
    - gpt

5. embedding
    - word embedding
    - collaborate filtering
    - word dimension

## 비지도학습 알고리즘
# pca

람다를 이용한 오버피팅 문제가 해결이 안되는 경우
데이터가 가지고 있는 feature의 개수(차원의 수)를 축소할 수 있다

우선 가지고 있는 트레이닝셋 데이터를 피쳐스케일링하고, 평균을 0으로 맞춘다
이제 목표하는 축소된 차원의 수를 k라고 하자

k = 1`
svd함수로 데이터의 공분산행렬 생성 
차원축소가 완료된 데이터 = 공분산행렬'(k번째 열까지) * 데이터
차원축소된 데이터의 복구 = 공분산행렬(k번째 열까지) * 데이터(대칭행렬의 역행렬은 transpsse와 같기 때문)
s행렬을 활용해서 정보의 retain정도를 확인, 99%보다 낮으면 k를 증가시켜서 반복

트레이닝셋의 데이터차원을 축소하는 k와 공분산행렬을 찾았다면
이제 그 파라미터를 사용해서 cv셋과 t셋에 사용한다



2. k-means
3. pca
4. gaussian abnormaly > svm

## 강화학습을 이해하는 세부관점 정리중
1. 주어진 문제를 mdp로 구성
    - mdp 핵심 구성요소
        - 상태: 유한 / 무한
        - 행동: 상태를 변수로 가지고 행동을 결과로 가지는 확률분포함수를 정책이라고 한다
        - 보상

    - mdp 하이퍼 파라미터
        - 상태변환확률 : 행동에 의해 어떤 상태에서 다른 상태로 변화할 확률들의 집합
        - 감가율

    - mdp 풀이 = 현재 정책에 따른 real / semi-real v(s) 얻기 + 얻은 v(s)로 정책 업데이트 >> 반복 = optimal v(s) + optimal policy(s) 획득
        - 어떤 타임 t에서의 어떤 상태 s에서, 현재 정책에 따른, 감가율을 고려한, 미래에 얻을 수 있는 모든 보상의 합 = gt
        - 어떤 타임 t에서의 어떤 상태 s에서, 현재 정책에 따른, 감가율을 고려한, 미래에 얻을 수 있는 모든 보상의 합의 기대값 = v(s)

2. mdp를 풀이하기 위한 value and policy 업데이트 방법
    - v(s) 업데이트 방법1
        - 모든 상태 s가 유한한 경우, 모든 v(s)를 직접 계산할 수 있다
        - gt의 정의로부터 아래의 식을 유도한다
        - v(s) = 시그마(행동 확률 * (reward + 감가율 * v(s'))
        - 위의 수식에서, 현재 상태 s와 어떤 행동 a에 대해 얻어지는 상태 s'의 관계는
        - s에서 a를 해서 s'이 됨
        - 과 같으므로, 우리는 v(s')을 s와 a에 대한 함수로도 생각할 수 있다. 이러한 함수를 q 함수라고 한다면 다음이 성립한다
        - v(s') = q(s, a)
        - 따라서 상태 s는 가능한 행동 a들에 대해 여러 q(s, a)를 가진다
        - 따라서 위의 식을 다시 쓰면 아래와 같다
        - v(s) = 시그마(행동 확률 * (reward + 감가율 * q(s, a))
        - 여기까지 따라왔다면, 위의 수식을 조금 더 일반화시키면 아래와 같음을 이해할 수 있다
        - q(s, a) = 시그마(행동 확률 * (reward + 감가율 * q(s', a'))
        - 결론적으로 아래의 두 수식을 활용한다
        - v(s) = 시그마(행동 확률 * (reward + 감가율 * v(s')): 정책 이터레이션, 가치 이터레이션
        - q(s, a) = 시그마(행동 확률 * (reward + 감가율 * q(s', a')): sarsa, q러닝, deepSarsa, dqn

    - v(s) 업데이트 방법2
        - 모든 상태 s가 무한한 경우: 모든 v(s)를 직접 계산할 수 없다: mc(몬테카를로) 방법을 사용한다
        - 에피소드1: 지나온 경로를 기록하고, 경로에 따른 gt를 각 스테이트에 기록하자
        - 에피소드2: 지나온 경로를 기록하고, 경로에 따른 gt를 각 스테이트에 기록하자
        - 에피소드 반복
        - 이제 기록된 모든 스테이트에 따른 gt 기록들의 평균을 구하자 = v(s)
        - 이 평균은 지금 정책에 따라 얻어지는 참 가치함수에 수렴한다
        - 그런데 이렇게 하려면, 에피소드 1000번동안 있었던 모든 걸 다 기록해야 되는데, 이건 메모리가 많이 든다
        - 따라서 테크닉을 사용, 임의의 n번째 에피소드가 종료되면 즉시 v(s)를 업데이트 해도
        - 1000번의 에피소드를 마치고 v(s)를 업데이트한 결과와 같도록 수식을 구성한다
        - 업데이트할 v(s) = 기존의 v(s) + 1/n * (현재 gt - 기존의 v(s))

    - v(s) 업데이트 방법3
        - 모든 상태 s가 무한한 경우 : 모든 v(s)를 직접 계산할 수 없다 : td(temporal difference) 방법을 사용한다
        - full 에피소드 없이, 액션에 일어날 때마다 v(s) 실시간 업데이트
        - 업데이트할 v(s) = 기존의 v(s) + learning_rate * (현재 받는 리워드[에피소드 필요ㄴㄴ] + r * v(s')[에피소드 필요ㄴㄴ] - 기존의 v(s))
        - 반복해서 현재 정책에 따른 참 가치함수값을 얻고, 정책을 업데이트하고, 이를 반복한다

    - v(s)와 정책 업데이트 방법1 - 1
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 반복 업데이트: 모든 상태의 현재 정책에 따른 real v(s)를 얻는다
        - 얻어진 real v(s)에서 그리디를 적용해서 정책 1회 업데이트
        - 위의 2과정을 반복

    - v(s)와 정책 업데이트 방법1 - 2
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 1회 업데이트: 모든 상태의 현재 정책에 따른 semi-real v(s)를 얻는다
        - 얻어진 real v(s)에서 그리디를 적용해서 정책 1회 업데이트
        - 위의 2과정을 반복

    - v(s)와 정책 업데이트 방법2
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 반복 업데이트: 모든 상태의 현재 정책에 따른 real v(s)를 얻는다
        - 위의 업데이트에서 max v(s')을 사용하면 그리디를 적용하는 효과가 있기 때문에, 정책을 따로 업데이트 할 필요가 없다
        - 위의 과정을 반복

3. mdp 문제 해결
    - 정책 이터레이션
        - 모든 상태 s가 유한한 경우: 정책 이터레이션
        - 현재 정책에 따라 얻어지는 참 가치함수값을 구하기 위해서 v(s) 업데이트를 여러번 반복한다
        - 이렇게 반복해서 얻어진 참 가치함수를 통해 정책을 업데이트 한다
        - 업데이트 된 새로운 정책에 따라 얻어지는 참 가치함수값을 구하기 위해서 v(s) 업데이트를 여러번 반복한다
        - 이렇게 반복해서 얻어진 참 가치함수를 통해 정책을 업데이트 한다
        - 이 과정을 반복하면, optimal 정책망이 얻어진다
        - 정책망에 따른 대강의 가치함수 >> 대강의 가치함수에 의한 정책망 업데이트 를 반복해도 같은 결론으로 수렴하게 된다

    - 가치 이터레이션
        - 모든 상태 s가 유한한 경우: 가치 이터레이션
        - 가치함수를 업데이트 할 때, 평균이 아니라, 최대값을 골라서 업데이트 한다
        - 무한히 반복하면 가치함수 자체가 옵티멀 정책을 내제하게 된다

    - sarsa
        - 상태 s가 너무 많은 경우: sarsa
        - td + 가치 이터레이션
        - 업데이트할 q(s, a) = 기존의 q(s, a)
        - + learning_rate * (현재 받는 리워드[에피소드 필요ㄴㄴ] + 감가율 * max q(s', a')[에피소드 필요ㄴㄴ]- 기존의 q(s, a))
        - 다음 스테이트 s'에서 최고의 큐를 반환해주는 a'을 그리디하게 선택해서 업데이트를 먼저 하고
        - 실제 이동은 입실론 탐험이 반영되지 못한, 이미 선택된 a'에 의해 이동한다
        - 맥스를 선택하기 떄문에, 정책평가가 이미 가치함수에 반영된다
        - 정책평가 없이 반복적인 가치함수 업데이트!

    - q러닝
        - 상태 s가 너무 많은 경우: q러닝
        - 큐함수를 이용해서 다음 큐함수를 업데이트 하면, 뒤의 액션에 의해 앞의 액션이 영향을 받아 갇히는 현상이 발생할 수 있다
        - 따라서, 정책은 explore + 그리디를 조합해서 쓰는건 똑같은데
        - 큐함수를 업데이트할 때, 다음 큐함수로부터 업데이트 하지 않고, 다음 state에서 얻어지는 max q(s, a)만을 사용해서 업데이트 하자
        - 다음 스테이트 s'에서 최고의 큐를 반환해주는 a'을 그리디하게 선택해서 업데이트를 먼저 하고
        - 실제 이동은 입실론 탐험이 반영된 택한 경로에 의해 이동한다
        - 맥스를 선택하기 떄문에, 정책평가가 이미 가치함수에 반영된다
        - 정책평가 없이 반복적인 가치함수 업데이트!

    - reinforce : 정책을 gt에 의해 선택한다
        - 현재 정책 = 현재 네트워크
        - 정책 업데이트 = 네트워크 업데이트(by gt)
        - 입실론을 쓰지 않는다 : 모든 행동이 확률적으로 나오기 때문에, 확률뷴포에 따르는 선택을 한다

    - deepSarsa
        - 살사 알고리즘 : 큐함수 업데이트 + 정책 업데이트
        - 현재 큐함수 = 네트워크가 근사하는 함수
        - 큐함수 업데이트 = 네트워크 업데이트
        - td : 다음 행동을 미리 그리디하게 선택하고 업데이트

    - dqn
        - q러닝 알고리즘 : 큐함수 업데이트 + 정책 업데이트
        - 현재 큐함수 = 네트워크가 근사하는 함수
        - 큐함수 업데이트 = 네트워크 업데이트
        - td : sars'을 리플레이 메모리에서 랜덤하게 추출, 다음 q를 maxq 선택 후 업데이트
        - 타겟 네트워크와 업데이트 네트워크를 분리한다

    - a2c = reinforce + deepsarsa
        - value/ q : 네트워크
        - action : 네트워크
        - 큐 신경망 업데이트 = 다음 큐함수 - 밸류(td) 제곱을 최소화하는 방향으로 업데이트
        - 정책 신경망 업데이트( : 큐함수 업데이트를 가져다가 쓴다
        - 입실론을 쓰지 않는다 : 모든 행동이 확률적으로 나오기 때문에, 확률뷴포에 따르는 선택을 한다

    - a3c = reinforce + dqn
        - 여러 에이전트가 동시에 작동해서 리플레이 메모리 문제를 해결하고 글로벌 네트워크를 업데이트 한다
        - multi-step으로 만들어진 q를 쓴다
        - q에 엔트로피 수식을 추가해서 exploration 경향성을 추가한다
        
# 3년동안 생각한거 헛소리 모음(진짜 헛소리가 99.9%라서 이제부터 거르는 중...인데 언제 다 보지?...)
강화학습은 공간에 생명을 부여하고
(블럭이 어떤 공간에 도착하면, 그 공간에서 블럭이 얻어야 할 정보들을 공간이 저장해두고 또 알려줌)
(블럭이 얻어야 할 정보 : 주변 가까운 공간의 블럭들의 정보)
(지금 현재 공간이 추천해주는 나의 이동방향 :이 이동방향은 마지막 레벨 공간에만 적용해야 한다. 왜냐하면 이동방향을 기록하려면 차원의 수가 일정해야 하기 떄문에, 차원공간 자체를 바꿀 수 없기 때문)

지도학습은 블럭에 생명을 부여하고
(각각의 블럭을 수학적 구조체로 형상화)
(결합된 블럭은 새로운 구조체가 된다 : 새로운 구조체가 되는 과정을 고정네트워크로 만들기)
(새로운 구조체는, 기존 자식 구조체와 그 외부 성질은 동일하지만, 차이점이 존재한다)
(자식블럭과 부모블럭의 공통점 : 외부로 드러나는 속성의 종류)
(차이점 : 외부로 드러나는 속성이 더 세부적이다? 결합이 가능한 대상블럭의 수가 다르다?)
(나중에 더 상위레벨로 갈 때 이용되는 네트워크가 레벨별로 다르다? 같다?)

비지도학습은 이미지의 의미없는 정보들을 기본블럭으로 변경할 수 있게 해준다
(이미지를 10000개의 숫자로 변형한 후, 이 숫자들이 정상적인 고양이를 생성하도록 하는 기본블럭집합으로 변경시킨다)

노드 : 좌표
에지 : 스프링
에지에 연결된 스프링의 힘의 합이 0이 되도록 노드가 이동할 수 있다
각 노드끼리는 스프링의 상태에 따라 인력과 척력이 발생할 수 있다
그런데 이렇게 하면 노드의 공간상의 위치에 따라 인력과 척력이 생기기 때문에 노드 고유의 성질을 가질수는 없다

따라서 기본 블럭이 될 구조체는 공간으로부터의 힘 + 자체적으로 가지는 고유한 성질에 의한 힘 모두를 가져야 한다

여러 공간을 겹친다면?
스프링으로 된 인력 척력 공간은 거리에 따른 힘을 블럭에 부여할 수 있다 : 규칙 1 : 너무 가까우면 척력이 발생 + 균형을 이루고자 하는 성질
노드가 구조체인가? 아니지 그런데, 구조체정보의 일부가 될 수는 있다
구조체라는게, 여러겹의 공간차원에 걸쳐서 각자의 정보를 가지도록 해보자

1. 3차원 : 인력과 척력의 성질 >> 만약 이 구조체들이 모여도, 역시 동일한 형태의 규칙을 가질것인가?


1. 각 노드가 서로 당기는 힘이 있다고 가정
2. 그 힘이 매우 강력한 노드끼리 모이면 그 노드들의 집합면이 구성됨
3. 그러한 구조체는, 매우 좁은 공간에 많은 노드가 모여 있으므로, 주변 노드를 당기려는 힘이 매우 강할수밖에 없다 = 강력한 중력
4. 좁은 공간에 많은 노드가 모여 있는 상태는 큰 질량을 가진 것으로 이해할 수 있음
5. 시작에서 발생한, 서로 당기는 힘은 각 노드가 자신끼리 당기는 새로운 힘이 있는 것으로 이해할 수 있음?
6. 중력이 약한 이유는, 만약 한 공간 안으로 스프링이 모이게 하려면 강한 힘이 필요하지만, 결구 그렇게 당겨버리고 나면 외부의 모든 스프링들은 적당히 그 힘이 분산되어 서로 늘어나니까 중력 자체는 약해진다.

7. 이렇게 어떤 공간ㄴ 내부의 어떤 노드 집합체를 상상해보고, 그것을 하나의 구조체로 하자(공간 + 내부 집합체 다 해서)
이 구조체가 실제 공간을 휘젓고 다니려면, 공간이 또 있어야되는뎅? 아니다
8. 스프링은 계속해서 자신의 연결을 끊고, 다시 재연결할 수 있다. 즉, 노드는 자신의 스프링 연결 상대방을 계속해서 교환할 수 있다!

9. 집합체가 되면, 그 표면 외부의 모든 노드의 스프링이 미치는 범위를 그 구조체의 특성으로 할 수 있다
10. 










1. 힘에 의해 변형되는 스프링 이산공간 생성
2. 공간집합의 각 노드에 가속벡터 추가
3. 스프링이 자유롭게 재연결되는 분리된 노드들의 집합 공간
4. 노드끼리의 스프링으로 인해 발생하는
아니다
공간이 중첩되어 있다고 생각하는게 맞는듯
공간이 레이어처럼 되어있다고 생각하고
각 레이어마다 스프링의 종류와 길이가 다르고
하위 레이어의 결과를 상위 레이어의 노드가 가져가는 식으로 하자
방법이 이거뿐임

스프링이 그떄그떄 연결되는게 아니라
노드의 위치가 먼저 결정이 되고
그 레이어 공간의 스프링 표준길이에 따라 노드간의 스프링의 탄성 및 인력 척력이 결정되는 구조

하위 레이어에서 상위 레이어로 넘겨 줄 정보
= 인력 및 척력의 크기: 이 크기가 연결되는 스프링의 종류에 따라 정확하게 이산화되야한다?

만약 차원을 3이 아니라 6차원으로 하면 가능한 수직방향의 스프링 개수가 12개니까
스프링의 상태에 따른 노드의 합력 상태가 더 다양하게 이산화될 수 있다

합력이 0가 되는 내부의 모든 노드의 정보는 제거하고
합력이 0이 되지 않는 표면 노드만 싸그리 모아서 상위 레이어로 정보를 보내면 됨
상위 레이어로 표면노드의 각 노드가 가지는 합력상태, 표면 형태?를 보내면 될 거 같은데
대체 표면 형태가 뭐지? = 노드의 집합체를 대표하는 어떤 형태집합의 특성
표면과 힘이라는 2개의 상태를 상위 레이어로 보내면 된다?

표면을 어떻게?: 모든 각각의 집합체에 주변 집합체의 표면을 탐색할 수 있는 권한을 준다 or 각각의 집합체가 자신의 표면 정보를 외부로 드러낸다
진짜 물리적인 표면이 있어야 하나? 표면은 그냥 정보일 뿐인데, 그냥 정보로 자신의 표면정보를 수로 노출하면 될거같다
>> 소수????

대신 각 집합체들은 이동이 가능해서, 서로 당기는 것ㄱ끼리는 공간상에 가까운 위치에 있을 수 있지 않을까
공간상 가까운 곳에 존재하면, 모든 공간을 탐색하지 ㅏㅇㄶ고 자신의 근처만 탐색하는 방식으로 정보를 연결할 수 있다

다만 최초에 그럼, 어떻게 멀리 떨어져 있어도 가깝게 배치될 수 있을까?
그건 공간이 해준다!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!?????????????????????????????????
집합체가 정보를 공간으로 노출하면, 공간이 집합체를 알맞은 위치로 이동시켜준다 or 재배치해준다

1. 최초 레이어는 미리 만들어 둔다
2. 정보덩어리를 가져와서 2번째 or 3번째 레이어의 노드에 1대일 대응시킨다
3. 레이어상에서 공간과 그
4. 







1. 마지막 레이어는 최종 노드들끼리 관계성을 바탕으로 저장되어 있는, 유동성이 존재하지만 적은, 그러한 공간이다
(워드벡터의 아이디어를 쓰면 공간의 초기 상태를 구현하기 쉽다)
2. 마지막 레이어는 일종의 하드디스크와 같다
3. 새로운 정보가 들어오면 이 레이어에 정보를 쓴다
4. 기존의 워드벡터는 각 노드끼리의 관계성을 흐름으로 나타내는 네트워크 하나가 있었다
5. 이걸 바꾼다
6. 각 노드 자체가 외부로 노출하고 있는 정보들이 존재하며, 그 정보의 존재에 의해 노드는 유동적으로 움직인다
7. 이 정보를 보고 노드의 위치를 조금씩 움직여주는 네트워크가 존재할 것인가 vs 이 노드의 위치를 움직여주는 공간이 존재할 것인가 vs 둘 다?!

표면이든, 인력이든, 척력이든, 결국 서로 합을 맞추기 위한 정보에 불과하다
그 레이어상에서 규칙성이 존재할 수 있는 정보라면 어떤 정보든 가능
소수, 표면, 숫자곱, 인력척력, 무엇이든 가능하다

레이어가 종류별로 있다
1. 정보 저장 레이어(1층부터 9층까지?)
2. 정보 인식 레이어(새로운 정보를 보면 적당히 분해한 후, 재조합을 거치다가, 마지막 레이어에서 정보 저장 레이어로 일대일 이동)

모든 레이어는 동일한 특징을 가진다
1. 레이어 공간은 자체적으로 비슷한 노드끼리는 모으고, 다른 노드가 오면 다른 곳으로 보내는 일종의 장?을 가지고 있다
2. 

정보 저장 레이어도 종류별로 있다
1. 추상 정보 저장 레이어
2. 

여러 레이어가 동일한 계급이면 각 레이어끼리의 정보를 조합한 최종 레이어가 있어서 여기에서 여러가지 결정남 = 연산레이어?
그니까, 정보 저장 레이어에서 모든걸 다 계산할 수는 없고
연산 레이어(일종의 메모리)가 있어서, 이 레이어에서 지금 필요한 구조체만 올린 다음 연산한다 일종의 메모리임



새로운 정보(고양이)가 들어온다면
이 정보를 분해해서 분석하다가, 어떤 레이어에 들어오면


블럭의 집합이 만족해야 할 표면이 존재하면 ? 마치 테트리스처럼?














강인공지능 개발
1. 힘에 의해 변형되는 스프링 이산공간 생성
2. 공간집합의 각 노드에 가속벡터 추가
3. 스프링이 자유롭게 재연결되는 분리된 노드들의 집합 공간
4. 노드끼리의 스프링으로 인해 발생하는
아니다
공간이 중첩되어 있다고 생각하는게 맞는듯
공간이 레이어처럼 되어있다고 생각하고
각 레이어마다 스프링의 종류와 길이가 다르고
하위 레이어의 결과를 상위 레이어의 노드가 가져가는 식으로 하자
방법이 이거뿐임

스프링이 그떄그떄 연결되는게 아니라
노드의 위치가 먼저 결정이 되고
그 레이어 공간의 스프링 표준길이에 따라 노드간의 스프링의 탄성 및 인력 척력이 결정되는 구조

하위 레이어에서 상위 레이어로 넘겨 줄 정보
= 인력 및 척력의 크기: 이 크기가 연결되는 스프링의 종류에 따라 정확하게 이산화되야한다?

만약 차원을 3이 아니라 6차원으로 하면 가능한 수직방향의 스프링 개수가 12개니까
스프링의 상태에 따른 노드의 합력 상태가 더 다양하게 이산화될 수 있다

합력이 0가 되는 내부의 모든 노드의 정보는 제거하고
합력이 0이 되지 않는 표면 노드만 싸그리 모아서 상위 레이어로 정보를 보내면 됨
상위 레이어로 표면노드의 각 노드가 가지는 합력상태, 표면 형태?를 보내면 될 거 같은데
대체 표면 형태가 뭐지? = 노드의 집합체를 대표하는 어떤 형태집합의 특성
표면과 힘이라는 2개의 상태를 상위 레이어로 보내면 된다?

표면을 어떻게?: 모든 각각의 집합체에 주변 집합체의 표면을 탐색할 수 있는 권한을 준다 or 각각의 집합체가 자신의 표면 정보를 외부로 드러낸다
진짜 물리적인 표면이 있어야 하나? 표면은 그냥 정보일 뿐인데, 그냥 정보로 자신의 표면정보를 수로 노출하면 될거같다
>> 소수????

대신 각 집합체들은 이동이 가능해서, 서로 당기는 것ㄱ끼리는 공간상에 가까운 위치에 있을 수 있지 않을까
공간상 가까운 곳에 존재하면, 모든 공간을 탐색하지 ㅏㅇㄶ고 자신의 근처만 탐색하는 방식으로 정보를 연결할 수 있다

다만 최초에 그럼, 어떻게 멀리 떨어져 있어도 가깝게 배치될 수 있을까?
그건 공간이 해준다!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!?????????????????????????????????
집합체가 정보를 공간으로 노출하면, 공간이 집합체를 알맞은 위치로 이동시켜준다 or 재배치해준다

1. 최초 레이어는 미리 만들어 둔다
2. 정보덩어리를 가져와서 2번째 or 3번째 레이어의 노드에 1대일 대응시킨다
3. 레이어상에서 공간과 그
4. 







1. 마지막 레이어는 최종 노드들끼리 관계성을 바탕으로 저장되어 있는, 유동성이 존재하지만 적은, 그러한 공간이다
(워드벡터의 아이디어를 쓰면 공간의 초기 상태를 구현하기 쉽다)
2. 마지막 레이어는 일종의 하드디스크와 같다
3. 새로운 정보가 들어오면 이 레이어에 정보를 쓴다
4. 기존의 워드벡터는 각 노드끼리의 관계성을 흐름으로 나타내는 네트워크 하나가 있었다
5. 이걸 바꾼다
6. 각 노드 자체가 외부로 노출하고 있는 정보들이 존재하며, 그 정보의 존재에 의해 노드는 유동적으로 움직인다
7. 이 정보를 보고 노드의 위치를 조금씩 움직여주는 네트워크가 존재할 것인가 vs 이 노드의 위치를 움직여주는 공간이 존재할 것인가 vs 둘 다?!

표면이든, 인력이든, 척력이든, 결국 서로 합을 맞추기 위한 정보에 불과하다
그 레이어상에서 규칙성이 존재할 수 있는 정보라면 어떤 정보든 가능
소수, 표면, 숫자곱, 인력척력, 무엇이든 가능하다

레이어가 종류별로 있다
1. 정보 저장 레이어(1층부터 9층까지?)
2. 정보 인식 레이어(새로운 정보를 보면 적당히 분해한 후, 재조합을 거치다가, 마지막 레이어에서 정보 저장 레이어로 일대일 이동)

모든 레이어는 동일한 특징을 가진다
1. 레이어 공간은 자체적으로 비슷한 노드끼리는 모으고, 다른 노드가 오면 다른 곳으로 보내는 일종의 장?을 가지고 있다
2. 

정보 저장 레이어도 종류별로 있다
1. 추상 정보 저장 레이어
2. 

여러 레이어가 동일한 계급이면 각 레이어끼리의 정보를 조합한 최종 레이어가 있어서 여기에서 여러가지 결정남 = 연산레이어?
그니까, 정보 저장 레이어에서 모든걸 다 계산할 수는 없고
연산 레이어(일종의 메모리)가 있어서, 이 레이어에서 지금 필요한 구조체만 올린 다음 연산한다 일종의 메모리임



새로운 정보(고양이)가 들어온다면
이 정보를 분해해서 분석하다가, 어떤 레이어에 들어오면



1.
- 현실세계 : 객체들의 정보 + 객체들의 관계 + 객체들의 매핑경로
- >> 인간의 매핑 : 단어 + 문장
- >> 인공지능의 매핑 >> 언어차원공간에 객체들의 정보 + 객체들의 관계 매핑 / 객체들의 매핑경로 네트워크 생성
- 언어차원[개념]공간을 생성하는 방법은 다음과 같다
- 임베딩1
- 사전순으로 나열된 단어들이 있다. 이를 사전공간이라 한다
- 이제 사전공간 >> 임베딩공간(개념공간임)으로 매핑을 하고싶다
- 이제 이 임베딩공간에서 매핑일 일어나는 네트워크를 만들것이다.
- 그러나 현재 순서가 반대로 되어있다 즉,
- 임베딩공간은 정의했으나, 임베딩벡터가 존재하지 않고
- 반대로 그 임베당공간상의 매핑결과는 인간이 생성한 수많은 문장에 의해 이미 결과로 얻어져 있는 상태가 된다
- 따라서 다음과 같은 방식을 제안한다
- 첫번째 임베딩벡터를 임의로 설정하고 >> 다음 임베딩벡터가 무엇인지 네트워크로 예측하고 >> 트레이닝셋에 맞도록 학습
- 이렇게 하면 절대적인 임베딩벡터의 좌표는 얻을 수 없으나, 두 임베딩벡터의 상대적인 차이를 알게되고
- 이러한 일을 모든 단어에 반복시행하면, 결국 모든 임베딩 벡터로 조합된 하나의 개념공간이 형성된다
- 정리 : 임베딩단어 몇개 >> 타겟은 결정됨 >> 사전공간의 모든 벡터에 대해 타겟이 가능한지 확률추측
-  >> 소프트맥스로 최고확률을 가지는 단어를 결정 >> 답이 타겟과 다르면 같아지도록 트레이닝
- 임베딩2
- 하나의 임베딩벡터를 정하고, 이 임베딩벡터의 다음 매핑벡터를 다른 모든 사전벡터에 대해 비교하는게 아니라
- k개의 제한된 사전벡터를 분포로 잡고 1개는 1, 나머지 벡터는 랜덤으로 불러서 0으로 매핑하는 네트워크를 만들자
- 이 네트워크를 훈련시키면, 확률적으로 k개는 0일 확률이 크기 떄문에, 설마 0이 아니었다 해도 다음 훈련으로 상쇄되니까
- 결국은 원하는 개념공간을 얻는다
- 정리2 : 하나의 임베딩단어 >> 참인 타겟1개와 네거티브인 타겟k개를 결정 >> 즉, 사전공간의 k + 1개의 벡터에 대해
- 참 거짓 유무를 최대한 맞추도록 소프트맥스로 트레이닝
- 이제 생성한 개념공간을 정의해줄 차원축을 정의한다
- 개념공간의 차원축 뽑아내기 : 개념공간을 형성했을 때, 각 특정 차원이 극단적인 값을 가지고 나머지는 아닌 벡터를 뽑으면 된다
- 이미지만으로는 비어있는 감각에 대한 정보가 없어서 벡터가 불완선하고
- 언어만으로는 비어있는 이미지에 대한 정보가 없어서 벡터가 불완전하고
- 두 벡터공간을 하나로 만들 수 있다면 속성벡터가 완성되는데
- 두 벡터공간을 하나로 하지 말고 연결만 하는게 지금의 방법인가?
- 강화학습의 액션생성법 : 유전 알고리즘으로 하자!!!!!!!!!!!!!!
- 두 이미지간의 일치성에 의한 벡터소환을 어떻게 하는가? >> 샴 네트워크의 원샷러닝을 활용하자!
- vae에서 가우시안이 아니라 가우시안의 합으로 근사
- 임베딩3
- n차원 가우시안 분포가 있다고 하자
- 가우시안 분포상 최대 확률을 가지는 가각의 집합셋ㅅ이 있다
- 이 하나의 집합데이터를 벡터로 가정한다
- 최대 확률만 가지는 각각의 랜덤변수 >> 하나의 벡터 : 임베딩 과정
- 무엇을 임베딩하는가? : 임베딩한 대상끼리의 일련의 매핑경로가 규칙적으로 존재하는 대상
- 이미지를 생성하는 벡터는 임베딩 대상이 되는가? : no!
- 언어를 만드는 단어는 임베딩이 되는가? yes!
- 소리를 생성하는 벡터는 임베딩 대상이 되는가? : no!
- 음악을 만드는 소리는 임베딩이 된다! : yes!
- 임베딩 대상이 되는 벡터란, 그 벡터의 연속적인 매핑에 대한 충분한 정보가 있어서, 한 벡터가 다른 벡터의 위치를 정할 수 있는 경우다
- 객체를 [인식]하는 일 : 임베딩이 의미가 없다
- 객체끼리의 [관계] : 임베딩이 필요하다
- 상태는 하나의 현실인데, 이 상태를 표현하기 위해 순서가 있는 단어를 쓰는데
- 단어는 각 객체와, 객체의 상태를 표현하기 떄문에, 연관관계가 있고, 임베딩 대상임
- 하나의 이미지는, 생성해 내는데 서로간의 관계가 있는가?
- 이미지를 생성하는 각 속성끼리의 연관관계는 : 객체간의 구별 + 3d 인식 + 각 객체의 특징
-
- 생각 : 정보 + 정보집합의 매핑
- 정보는 벡터다 : 정보를 구성하는 개념을 차원축으로 가지는 벡터공간을 생성한다
- 정보집합의 매핑은 네트워크다 : 상상네트워크와 현실네트워크의 차이를 최소화한다
- 벡터와 네트워크는 동시에 존재할때 의미가 있다
- 1차 후보개념벡터공간 임베딩 및 생성하기
- >> 1000차원 랜덤벡터공간을 생성한다
- >> 랜덤한 인풋데이터가 주어진다
- >> 인풋데이터에는 [감각]이라는 정보가 손실되어 있기 때문에, 매핑이 어렵다
- >> 인풋데이터를 1000차원 랜덤벡터공간에 [분할된] 벡터[집합]으로 매핑하고 재생성한다
- >> 재생성 과정이 잘 일어나게 되면 최초의 1000차원 개념벡터공간과 네트워크가 생성된다
- >> 분할된 벡터집합은 네트워크로 동시호출될 확률이 높다?
- >> 분할된 개념벡터집합을 즉각적으로 트레이닝셋으로 사용해서 싸이클 네트워크를 훈련
- >> 싸이클 네트워크?
- >> 중요! : 새로운 인풋데이터를 받으면 기존의 임베딩 상태를 refine할 수 있어야 한다
- 2차 refine개념벡터공간 임베딩 및 reine
- >> 1000차원 후보랜덤벡터공간에서 다양한 벡터집합셋을 뽑아서 [상태를] 생성하는 트레이닝
- >> 감별자가 필요한데...
- >> 감별자 = 문장셋?
- >> 감별자를 생성하자
- 어떻게 기존에 생성된 차원공간을 업데이트 하는가?
- 속성을 기존 차원공간에 추가하거나 업데이트할 수 있을까?
- 또는 가지고 있는 차원공간과 새로운 방법으로 만들어진 차원벡터의 유사성을 찾아 업데이트 할 수 있는가?
- 사과 이미지를 속성벡터로 전환한다
- 사과는 달다라는 문장을 인풋으로 받는다
- 달다라는 속성벡터를 찾는다
- 달다라는 속성벡터를 차원축으로 해서 사과의 속성벡터의 차원축 값을 변경하면 되는데
- 만약 달다라는 속성벡터가 차원축이 아닌 경우라면?
- 사과속성벡터에 달다라는 속성벡터를 더한 새로운 벡터롤 사과속성벡터로 재매핑하면 벡터가 리파인된다
- m차원 공간의 벡터에서 n차원 부분차원의 속성들을 해석하는 것이다, 즉 부분정보란 부분차원공간의 벡터다
- 만약
- 사과가 졸라 달다
- 라는 문장이 정보로 제시되면
- 사과속성벡터 + 달다벡터 * 졸라벡터가 의미하는 길이정도
- 만약 졸라도 하나의 벡터라고 가정하면 어떤 속성인가?
- 졸라 벡터는 아마 정도를 나타내는 차원축에 아주 작은 길이를 가지는 벡터일 것이다
- 어떤 정보던지 인풋이 들어오면 p차원 벡터로 일단 나타내는데
- 매치되는 정보가 없다면 다시 차원축을 추가해서 p + q로 하고
- 만약 언어 호출에 의해 겹치는 정보가 인풋으로 들어오면 차원축의 하나를 제거(공간을 겹치면 된다)
- 차원축을 부르는 주소가 필요?
- 사과이미지벡터를 생성해내는 이미지속성벡터가 있다
- 이 벡터는 속성을 표현하는 100차원 공간에 있다고 하자
- 이제 사과라는 단어를 호줄자로 등록하려고 한다
- 1만차원의 사전식 단어차원공간이 있다고 하자
- 사과라는 단어와 사과속성벡터를 단순연결하자(오토 인코더로 매우 단순하게 연결하는게 좋다)
- 이제 사과는 녹색이다 라는 문장을 인풋으로 받는다
- 호출자 사과단어에 의해 사과이미지속성벡터가 호출된다
- 녹색이다라는 단어는 호출될 벡터가 아직 없다
- 정보를 모르니까 걍 무조건 녹색이라는 단어가 호출할 이미지벡터를 새로운 차원축으로 결정해버린다
- 이제 녹색단어가 호출하는 새로운 이미지공간의 차원축이 생겼다
- 사과벡터가 존재하는 공간에 차원축이 추가되고, 벡터의 방향도 추가된 녹색의 축으로 이동하도록 refine한다
- 이제 이미지 녹색벡터를 생성하는 이미지 속성벡터가 있다고 하자
- 여기에 녹색이라는 단어를 호출자로 등록하자
- 녹색이라는 단어는 이미 위 과정에서 녹색을 하나의 차원축으로 등록했기 떄문에
-
- 사과는 맛잇다가 들어온다면?
- 맛잇다가 호출하는 속성벡터가 없다면 일단 맛잇다를 속성벡터의 차원축으로 추가하고
- 겹치는 정보, 즉 맛있다라는 단어가 특정 이미지를 호출할 수 있다는 정보를 받기 전까지는 상태를 유지하면된다
- 아
- 추가할 때, 이게 꼭 차원의 축이라는 전제는 없는거니까
- 그냥, 차원의 축을 하나 늘리고, 새로운 차원에 꼭 값을 가지기만 하면 나머지 차원의 축에서는 랜덤값을 가지도록 해서 새로운 벡터를 만들자
- 이 때, 기존의 차원공간이 아니라 새로운 축을 반드시 하나 더 만들어야 한다? 왜? 이러면 뭐가 좋은걸까?
- 왜냐하면 이미지를 생성하는 차원공간에는 절대로 맛있다를 의믜하는 벡터를 표현할 차원축이 없는것이 자명하기 떄문이다
- 만약 녹색이다라는 벡터엿다면, 기존의 차원공간 내부에서 적당히 벡터를 정의하고 사과벡터에 더한 후 refine되어도 상관없다
- 그러나 존재하지 않는 속성이었다면? 정보의 손실을 막기 위해서는 필연적으로 차원축이 추가된 하나의 벡터를 생각해야 한다
- 만약 그것이 이미 존재하는 속성이었다면, 정보가 겹치면
- 겹치는 두 벡터를 정확하게 이등분하는 평면으로 모든 벡터를 같은 길이를 유지하면서 부채꼴 모양으로 이동해버리면 된다
- 부채꼴 모양으로 이동하면 벡터의 길이가 변하지 않으면서 원하는 평면으로 떨어지게 만들 수 있다
- 이 방법이 기존의 임베딩에 비해 뭐가 나을까?
- 일단 새로운 속성이 들어오면 추가할 수 있다는거임! 이게 제일 큼, 즉
- 이미지 속성, 감각 속성, 그 이외에 모든 속성을 전부 다 포함하는 단 하나의 속성공간을 만드는게 가능해 진다는거임
- 순서!
- 사과 그림 >> 사과 속성 >> 다시 사과그림
- 사과속성 >> 사전 사과
- 사과는 녹색이다 >> 사전사과 사전녹색 >> 사과속성호출, 차원축을 하나 늘리고 녹색속성랜덤벡터추가 및 사전녹색와 연결
- 녹색 그림 >> 녹색 속성 >> 다시 녹색 그림
- 녹색속성 >> 사전 녹색 연결하려고 보니
- 이미 사전 녹색이 연결자가 존재
- if 연결자가 존재하는 경우 :  원래 존재하던 녹색속성랜덤벡터와 현재 존재하는 녹색속성벡터의 두 벡터의 차를 법선벡터로 하는 평면 위로
- 존재하는 모든 벡터점을 회전시켜 이동한다
- 이동하면 좌표계가 바뀌어서 축을 제거할 수 없네 안되겟다
- 새롭게 추가된 차원축에 값을 0이 아닌 값으로 가지는 모든 벡터를 전부 다 호출한 다음
- 그 벡터에 다음과 같은 연산을 하자
- 현재 벡터에서 제거할 차원을 제거한 후 + 연결자벡터방향단위벡터 * 제거된차원축에서 가졋던 값
- 이렇게 하면 좌표계를 수정하지 않고 원래대로 벡터를 돌려놓을 수 있다
- if 연결자가 굉장히 거의 흡사한 경우 : 이 경우도 이동시키자
- 이렇게 하면 어떠한 것이 가능한가?
- 데이터를 매우 대강 던져도 학습할 수 있다
- 아직 존재하지 않으면 차원축을 추가하고 랜덤하게 데이터를 늘려나가고
- 기존의 학습데이터가 들어와서 겹치면 차원의 수를 내린다
- 이제 이렇게 차원공간의 속성벡터가 만들어 지는데
- 예를 들어 이미지가 통쨰로 들어온다고 치면?
- 이미지에 존재하는 모든 객체를 표현하는 속성벡터를 죄다 호출하자
- 이 이미지를 설명하는 문장으로 변환가능
- 반대로 문장을 주주면
- 속성벡터를 모조리 호출하자????????
- 이 벡터의 적당한 집합?체를 하나의 단위로 생각하자????????
- 어떻게 적당한 집합체가 호출될까??
- 이 벡터끼리 상호호출할 수 있는 네트워크 하나가 필요하다 : 세계의 관찰을 통해 학습된다
- 이 네트워크의 존재로 추론능력이 발생한다 : 집합셋에 빠져있는 속성벡터의 존재를 찾을 수 있게 된다
- rnn이 아니라, cycle을 만들어야 되는데
- 문장을 생성할 떄는 순서가 중요하지만
- 개념을 떠올릴때는 순서가 아니라 연관된 속성벡터의 집합을 모두 호출하는게 중요하다고 가정하자
- 이제 하나의 속성벡터가 주어지면, 이로부터 다른 연관벡터의 집합이 자동으로 호출되도록 네트워크를 만들자
- 이 네트워크의 집합을 이제 멀티매핑해야한다
- lstm이 이미 벡터셋의 멀티매핑이네?
- 멀티매핑인데, 서로가 서로에게 영향을 주는 멀티매핑이네
- rnn 구조를 여러쌍 만들고
- 최종 아웃풋이 나오면, 그 아웃풋을 다시 넣고 다음 문장을 산출?
- 스타화면을 보면
- 객체를 인식하고
- 각 객체의 모양을 본 경우 : 모양을 보고 비슷한 객체를 추론 >> 건물 >> 건물속성벡터 >> 속성이해
- 이 속성벡터를 호출하는 주소값을 역으로 부른게 문장이다
- 일단 객체를 인식해서 그 객체를 대표하는 속성벡터들을 뽑아야된다
- 그럼 마리오는 어떻게 한거지?
- 마리오 객체를 인식하지 않았잖아
- 객체인식 없이 그냥 화면이 주는 속성정보를 액션으로 차원축을 일일히 조절하면서 경로를 찾았다?
- 대체 액션이 뭐지?
- 이미지가 나타내는 속성벡터 집합을 뽑았다고 가정해보자
- 그게 건물, 유닛을 나타낸다고 해도
- 그 상태를 어떻게 변환해야할지 어떻게 알아?
- 멀티매핑은 왜 하는걸까?
- 서로서로의 벡터는 다른 벡터의 매핑에 영향을 주는가?
- 일단 준다고 가정만 하고 생각해보자 / 아닐수도 잇는듯?
- 만약 슈퍼마리오 화면을 생성하고 마리오의 움직임을 만들어 내는 속성벡터가 존재한다고 하자
- 이 속성벡터가 5개라고 하자
- 어떤 벡터는 마리오의 움직임 / 어떤 벡터는 버섯의 움직임을 만들어 낸다고 하자
- 두 벡터는 서로 견제할 수밖에 없는 차원속성이 있다
- 이 벡터집합이 목표하는 어떤 상태벡터가 있다고 하자
- 이제 서로의 벡터가 서로의 벡터를 제한하는데, 이를 직관적으로 생각하면
- 두 벡터가 매핑 도중에 특정 관계(각 + 방향)을 이루면 안되도록 제한하는 차원축의 집합들이 있다는 뜻이다
- 모든 차원축이 수직관계임을 감안하면, 각은 항상 90이기 떄문에, 연관된 것은 차원축 집합의 각각의 길이 뿐이다
- 이제 매핑을 하기 위해 네트워크를 정의하자
- rnn은 트레이닝셋을 통해 하나의 액션을만드는 네트워크다
- 트레이닝 셋이 없는 상태에서 액션을 만들고 네트워크로 벡터셋을 매핑하고자 한다
- 벡터셋을 랜덤하게 매핑하자 일단은
- 각각의 벡터셋을 매핑하면 벡터셋의 개수가 늘어날 수도 있다는게 문제네
- 각각의 벡터셋이 고를 수 있는 액션네트워크를 정의하자
- 어차피 모든 벡터는 다르지만, 같은 차원공간에서 움직이는 벡터니까, 네트워크를 정의하는 차원도 모두 동일하다
- 따라서, 모든 네트워크 매핑의 시작지점은 동일한 차원공간이다
- 이제 네트워크를 딥하게 만들고, 이 딥한 네트워크 하나하나를 액션 하나하나로 정의해야 한다
- 이미 생각했듯이, 이 딥한 네트워크로 만들어진 액션은, 가지고 있는 모든 속성정보로 사용해서는 안된다
- 일부 속성정보만 사용해서 만들어지는 5개의 네트워크는 현제 벡터가 무엇인가에 무관하게 작동하는 네트워크 5개다, 즉 인풋과 무관하다
- 이제 이 네트워크롤 각각의 벡터셋의 원소들이 마음이 드는 액션으로 선택해서 매핑된다
- 매핑되면 새로운 벡터셋이 생겨나고, 이 벡터셋들은 기존의 셋과 개수가 다를 수 있다
- 이제 이 벡터셋이 또 5개의 액션에서 원하는 액션을 골라서 다음 셋으로 매핑한다
- 이런 과정으로 매핑이 연속되는데, 목표에 도달하게 되면 하나의 매핑경로가 완성된다
- 여기서 다음과 같은 방법을 제안한다
- 5개의 초기액션으로 모든 매핑을 성공적으로 마친 경우, 코스트 함수가 제시되고 액센네트워크는 업데이트된다
- 여기까지 생성되는 결론은?
- 상태벡터공간 + 인풋으로 호출되는 멀티상태벡터셋 >> 상태벡터를 매핑하는 5개의 다중액션 >> 벡터의 멀티매핑
- 이제 이렇게 해서 만들어지는 최후의 매핑과정을 상상이라고 이름짓자
- 상상매핑을 정의할때는 불변정보가 없다고 가정해야 한다 >> 뭐든지 가능하도록 불변정보를 없앤 가상의 매핑
- 이제 현실에서의 매핑을 정의하자
- 현실의 매핑이란: 액션을 정의하는데, 가능한 액션과 불가능한 액션이 주어진 벡터셋에 따라 정해지는 것
- 어떤 액션을 시도했는데 벡터의 값이 전혀 변하지 않으면, 그 액션에서 사용되는 노드를 바로바로 버리는 실제 기법
- 이 때, 각 액션으로 인해 벡터가 매핑되는데, 벡터의 매핑결과, 값이 거의 변화하지 않는 벡터의 차원정보가 있을 것이다
- 이 불변차원의 정보를 어케 찾는가 하면 >> 어텐션을 쓴다
- 어텐션 파라미터를 써서 주목되지 않는 노드를 알아내고 확률적으로 매번 다음 샘플링때 선택될 확률이 감소하도록 만들자
- 어텐션 파라미터가 주목한 노드는 다음 샘플링때 선택될 확률이 증가하도록 한다
- 불변차원정보는 각 액션마다 다르게 선택하고 버린다
- 왜냐하면 어떤 액션에서는 다른 액션에서 불변으로 생각하는 정보가 불변이 아닐 가능성이 있기 때문이다
- 이렇게 하면 액션의 수는 일정하게 유지한 채로, 각각의 독립성을 유지하면서, 벡터를 최선으로 매핑하는 액션을 얻게된다
- 주의사항 : 한 액션네트워크에서 제외된 노드는 다른 액션네트워크 노드가 될 수 있다!
- 액션의 수가 늘어나면 강화학습이 어려워 지고, 액션의 수가 줄어들면 매핑의 완전성이 감소한다, 둘의 적당한 중간지점을 취해주는 것이 포인트다
- 이제 상상매핑경로와 현실매핑경로사이에 존재하는 차이함수가 있다
- 이 함수를 최소화 하는 방향으로 모든 현실경로를 업데이트ㅏㄴ다
- 상상매핑경로는 업데이트가 완료된 감별자임
- 현실속에서의 최선경로를 찾아낸다
- 객체정보를 넘길 수 있어야 됨
- 마리오는 어떻게 마리오 객체정보를 넘겼을까?
- 마리오는 처음부터 [주어지는]액션이 상태를 조절하는 액션이었구나
- 상태벡터를 얻을 필요도 없었네 >> 할 수 있는 액션이 이미 상태를 조절하니까
- 인풋을 받으면 임베딩 되어있는 개념벡터집합들이 호출된다
- 인풋 >> 이미지에 등장하는 객체 + 상태 + 단어 + 문장 >> 임베딩된 개념벡터집합 호출
- 매핑은 이미 네트워크로 완결되어 있고,
- 글이나 미경험 상태의 새로운 정보가 들어오면 임베딩이 추가될 수 있어야 한다
- 하나의 개념벡터가 호출되면 다른 개념벡터 집합이 자동으로 호출된다
- 어떤 개념차원들의 집합이 >> 다른 개념차원들의 집합과 연관되어 있다?
- 다중개념벡터동시매핑액션이필요하다
- 생성액션과 현실액션의 차이를 최소화하자
- 2.
- 이미지 개념공간을 만들어내기 위해, 개념벡터가 무엇인지, 기존의 3가지 네트워크에 대해 생각해보고 알아보자
- 1. 분류기
- 이미지 인풋 >> 중간개념차원 >> 다른 여러 고양이벡터와 평균적으로 일치하는지 확인
- 중간개념차원은 평균적인 고양이 이미지의 개념을 차원축으로 가진다
- 2. gan
- 개념차원이 될 후보공간에서 랜덤벡터 인풋 >>
- 생성된 결과물을 감별자가 확인 >> 감별자가 만족하도록 트레이닝 >> 개념벡터 형성
- 개념차원은 감별자가 인정하는 평균적인 고양이 이미지의 개념을 차원축으로 가진다
- 분류기의 중간개념차원과 gan의 중간개념차원은 어떤 차이가 있을까?
- gan의 개념벡터는 랜덤벡터에서 출발하기 떄문에, 평균적인 특성 이외에 좀더 디테일한 세부사항을 차원축으로 가질 수 있다
- 3. 오토인코더
- 이미지 인풋 >> 중간개념차원 >> 처음에 제시한 고양이벡터와의 일치도 확인
- 중간개념차원이 오로지 지금 제시한 이미지의 개념만 담는다
- 오토인코더 : 차원공간끼리 연결하는 네트워크로 쓴다
- 이미지를 받는다 >> 적당한 중간차원으로 차원의 수를 바꾸어 매핑한다 >> 다시 원래대로 복구한다
- 복구한 이미지가 처음 이미지와 같아지도록 트레이닝셋을 만들어 학습시킨다
- 놀랍게도 중간에 적당한 중간차원으로 설정한 벡터가 하나의 개념벡터가 되는데,
- 원래의 이미지가 가지고 있던 개념정보를 가지고 있는 개념공간을 형성할 수 있다
- 적당한 n개의 컨텍스트 이미지에 해당하는 이미지의 개념벡터를 생성하자
- 컨텍스트 이미지를 배경으로 하는 타겟 이미지의 이미지 개념벡트 시퀀스를 rnn으로 트레이닝하면 이미지 개념벡터의 매핑경로를 학습한다
- 비디오 영상을 프레임단위로 보여주고, 각각의 프레임을 생성하는 개념벡터를 생성하면
- 이 개념벡터 안에 '이미지의 연속성'이라는 개념이 들어가 있게 될 것이다.
- 물리법칙의 개념을 '인지'하도록 만들기 위해서는 물리현상이 일어나는 이미지의 변화를 보여주고 재생성하게 하면 된다
- 개념공간의 이미지개념벡터를 임베딩하고, rnn으로 매핑경로 네트워크도 생성
- 모든 이미지를 생성하는 고차원 벡터공간을 생성한다
- 만약 각 객체를 따로 생성하는 오토인코더를 가지고 잇다고 하고
- gan이 특정한 벡터를 생성한다고 하면
- 오토인코더는 적당한 벡터집합을 만들어낼 수 있다
- 벡터집합의 합 또는 매핑평면 >> 일련의 변화를 인식하고, 상태단어로 표현하는 과정이 필요
- 우선 객체와 배경을 분리해서 인식해야 함
- 아이디어1 : 그리드를 소실점으로 놓고 각 물체의 좌표를 알아내는 방법으로 트레이닝
- 소실점을 어떻게 찾는가? >> 컨볼루션을 쓰면 단방향 선특징을 찾아낼 수 있다 >> 선방향의 특징을 찾아서 소실점을 찾도록 트레이닝
- 아이디어2 : 이미지를 주고, 객체와 배경을 구분하도록 실제 움직이면서 확인하는 방법으로 트레이닝
- 이미지를 보여주고, 이미지의 각 위치와 객체를 파악하게 한 후, 실제로 이동하면서 형태를 알아볼 수 있도록 트레이닝
- 동영상을 이용해야 한다
- 자신이 움직이는 속도를 알면 상대까지의 거리를 이해할 수 있다
- 명사 형용사 주어 동사등을 구분하는 클래시파이어를 만들고 트레이닝
- 형용사와 동사를 임베딩차원의 각 축으로 미리 설정
- 설정한 축을 바탕으로 명사를 임베딩
- 트레이닝셋이 적어도, 단어벡터를 가져오면 트렌스퍼러닝으로 학습이 잘 된다
- 이게 무슨뜻이냐면, 어떤 객체에 대한 정보벡터가 충분히 있으면
- 그 객체들의 모음이 어떤 다른 정보를 던질 수 있다는 뜻이다
- 시각으로 객체와 배경을 인식하되
- 그 객체와 배경의 가지는 정보를 임베딩한 벡터를 소환할 수 있어야 하는데
- 그 임베딩 벡터에 쓰인 정보란 시각적 유사도 + 감각 + 경험데이터 등등이어야 한다
- 차원공간을 다루기 쉬운 모양으로 어케 만드냐면
- 그걸 2d공간에 뿌리고 cnn으로 다루자
-
- 사전공간을 >> 다른 공간으로 변형하기 위해 e매트릭스를 곱했다
- e매트릭스는 네트워크에 의해 산출된 벡터들의 집합셋으로, 변형된 공간 그 자체다
- 사전공간의 수직축끼리 매핑경로를 추정하는 방법을 써서(즉, 추가적인 벡터끼리의 정보를 알면)
- 수직축끼리의 공간의 모든 벡터를 새로운 공간으로 매핑할 수 있다
- 3.
- 작곡프로그램으로 할 떄 원핫벡터로 샘플링했는데
- 이걸 임베딩으로 바꾸면 되지 않을까?
- 소리개념벡터를 뽑아내서 소리개념공간 생성
- 언어, 이미지, 소리를 제외한 제4의 더미차원공간 필요?
- 이미지정보, 소리정보, 객체와의 상호작용으로 얻어지는 감각정보
- 객체와의 상호작용으로 사람은 지도학습을 한다
- 정보를 종합하고 >> 그것을 상태를 나타내는 벡터공간으로 매핑해야 한다
- 상태를 나타내는 벡터공간에서 정보를 조절
- 문제를 해결
- 인간
- 정보종합 >> 상태를 나타내는 언어공간으로 매핑
- 언어공간의 정보를 조절
- 문제를 해결
- 상태를 표현하는 객체의 속성차원이 다양하기 떄문에 피상적으로 드러나는 관계를 자세히 기술할 수 있으나 디테일을 다루지 못한다
- 인간
- 정보종합 >> 상태를 나타내는 수학공간으로 매핑
- 수학공간의 정보를 조절
- 문제를 해결
- 상태를 표현하는 객체의 속성차원이 매우 단순하기 떄문에 피상적인것 보다 디테일한 속성을 다룰 수 있다
- 다양한 이미지가 제시된다
- 그 모든 이미지를 생성할 수 있는 개념벡터를 만들어 낸다
- 이 개념벡터는 이미지가 가질 수 있는 모든 속성을 차원축으로 가진다
- 이 속성을 컨트롤하면 이미지를 바꿀 수 있다
- 같은 방법을 쓰자. 내가 원하는 것은 상태를 나타내는 벡터다. 상태벡터의 매핑 = 생각이다
- 객체 + 상태를 모두 포함하는 하나의 이미지를 인풋벡터로 하자
- 이 이미지벡터의 상태를 나타내는 모든 속성을 차원축으로 하는 어떤 상태벡터가 있다고 가정하자
- 그 상태벡터공간을 형성해야 하기 떄문에, 출발은 랜덤한 벡터다
- 그러나 이 벡터는, 상태를 나타내는 벡터여야 한다.
- 이 상태벡터가 옳은지 아닌지 우리는 객체와의 상호작용을 바탕으로 알지만, 인공지능은 간접적으로 상호작용을 해야한다
- 그런데, 인간이 상호작용 한 그 결과를 이미 정리한 상태매핑이 바로 문장이다.
- 따라서 객체와의 상호작용 >> 간접경험으로 언어벡터와의 일치성으로 바꾸어 생각하자
- 랜덤벡터 >> 네트워크 > 올바른 문장(문장은 이미 상태다)(단어의 상태들을 기술하는 벡터의 매핑)
- 랜덤벡터와 네트워크의 트레이닝을 gan으로 한다.
- 객체이미지 >> 객체이미지속성벡터  + 네트워크 >> 네트워크 + 언어속성벡터
- 감별자를 미리 만들어야 한다. 감별자는, 이미지와 문장의 순서쌍을 바르게 판단하는것을 트레이닝한 감별자라고 하자
- 만약 감별자를 이기게 된다면, 이 제너레이터로 감별자가 재학습을 시도한다. 그리고 재학습을 한 감별자를 제너레이터가 다시 속이는 과정을 반복하자
- 이제 생성된 초기벡터 = 상태벡터가 된다
-
- 다양한 객체들
- 생성네트워크 역시 강화학습으로 만들어보자
-
- 객체 >> 객체속성벡터
- 이미지
- 객체의 정보와 객체의 관계와 객체의 매핑을 나타내는 상태벡터를 만들어 낸다
- 이 상태벡터는 상태속성을 차원축으로 가진다
- 이 속성을 컨트롤하면 상태를 바꿀 수 있다
- 속성을 바꾼 결과를 가정해 보는 매핑 : 추론 및 상상
- 실제로 속성을 바꿔서 결과를 확인화는 과정 : 행동
- 4.
- 이제 생성된 3개의 차원공간에 존재하는 벡터끼리 결합해서 만들어진 하나의 정보벡터 생성
- 이를 위해 중요한 것은,
- 이러한 개념공간이 여러개 존재하는데, 각 개념공간의 차원축끼리의 연관성을 어떻게 만들어 내는가?
- 주어진 개념공간의 차원축을 나타내는(표현할 수 있는, 즉 하나의 값만 1에 가깝고 나머지가 대부분 0이 되는 벡터를 뽑는다)
- 이제 개념공간1의 차원축벡터가 300개, 개념공간2의 차원축벡터가 300개라고 가정하자
- 이 벡터끼리의 연관성이란, 어떤 차원축이 어떤 차원축을 잘 표현하는가라고 할 수 있다.
- 이제 이 300개의 차원축을 하나의 벡터로 생각해보자
- 따라서 주어진 벡터 2개를 연결하는 네트워크가 형성되어, 하나의 벡터가 다른 벡터를 호출할 수 있으면 된다
- 오토 인코더를 쓰자
- 개념공간 1의 벡터 >> 중간개념벡터1로 매핑 >> 개념공간 2의 벡터로 매핑 >> 결과를 >> 중간개념벡터2로 매핑 >> 개념공간1의 벡터
- 위와 같은 구조를 쓰고 코스트함수를 다음과 같이 정의하자
- 1. 중간과정에 존재하는 개념공간 2에서 발생하는 차이를 코스트1
- 2. 개념공간 1의 벡터로 다시 변환하고 난 차이도 코스트2
- 목표는 코스트1 + 코스트2를 최소화하는 방향으로 네트워크를 트레이닝하는 것이다
- 이 트레이닝의 결과로 얻어지는 결론은 다음과 같다:
- 언어공간의 단어벡터 1개가 호출되엇다고 가정하자
- 단어벡터의 각 차원이 가지는 값들에 중간개념벡터2를 네트워크로 연결하면 이미지벡터공간의 이미지개념벡터로 매핑된다
- 또는 이미지개념벡터로 매핑한 결과가 없다면, 그 매핑결과와 이루는 각이 가장 최소가 되는 벡터를 선택한다
- 이루는 각을 구하는 알고리즘이 필요함 : 이를 위해 각 벡터의 길이와 내적을 구하면 된다
- 이제 언어의 개념 + 이미지의 개념이 하나로 결합된 종합개념벡터가 생성되엇다
- 5.
- 이 종합개념벡터가 존재하는 차원공간 : 브리지차원공간 생성
- 브리지차원공간의 특성 : 3개의 차원공간이 가지고 있는 벡터정보들을 결합해서 하나의 벡터를 만든다
- 이제 브리지차원공간에서 정보벡터는 매핑경로를 그리며 움직이게 된다
- 중요! : 어떤 벡터를 제시할 때는, 매핑경로 또한 함께 제시해야 정보제한의 의미를 가질 수 있다.
- 이제 각 차원공간에서 제시하는 매핑경로 네트워크를 모두 더해서 다음 매핑경로를 제시한다
- 이게 아니구나
- 내가 스타크래프트 화면을 인풋벡터로 받았다 치면
- 이 벡터를 >> 내가 가지고 있는 차원공간의 개념들로 재조합해서 하나의 개념벡터를 만들어야 함?
- 6.
- 종합개념벡터의 매핑 네트워크를 형성해야 할 차례다
- 기억할 대전제 : 하나의 매핑네트워크는 하나의 액션이다
- 매핑네트워크는 [개념]을 다루는 네트워크다
- 5개의 액션 네트워크가 학습되고, 후반 레이어로 갈수록 하나의 액션네트워크가 컨트롤할 수 있는 노드(차원의 수)는 많아진다
- vs 후반으로 갈수록 액션네트워크들이 제한조건을 찾아내서 신경쓰지 않게되는 불변성노드들이 많아진다, 즉 다루어야 하는 신경쓸 노드의 수가 적어진다
- ai의 목표 : 문제가 주어진다 >> 해결한다
- 문제란 : 환경 + 제한조건 + 목표
- 환경이란? : 제한조건과 연괸된 정보가 아닌 벡터의 차원정보들을 모두 환경정보라고 하자
- 제한조건이란? : 벡터의 연속적인 매핑경로가 생성될 때, 특정 차원의 벡터값은 변할 수 없다
- >> 벡터의 매핑경로는 특정 차원에서 한 평면 위에서만 움직인다
- >> 제한조건이 늘어나면 벡터의 선택가능한 매핑경로는 줄어든다
- 목표란? : 제한조건을 만족하면서 벡터 매핑경로의 도착점이 목표벡터에 도달
- >> 벡터의 목표에 관련된 특정 차원의 정보가 목표값에 도달하는 것을 말한다
- 7.
- rnn : 순서대로 작동하는 일련의 과정이 데이터로 존재하면(지도)
- 벡터의 매핑경로를 학습한다 : 학습결과 = 다음에 등장하는 적당한 매핑경로를 확률적으로 샘플링
- 환경정보 : 현재 벡터의 정보 + 매핑경로상의 순서 및 위치
- 벡터의 제한조건 : 직접적으로 알기 어려운 경우, 올바른 매핑결과를 트레이닝셋으로 제공
- 제한조건이 숨어있어, 직접적으로 네트워크에 제시하기 어려운 경우에 사용한다
- 숨어있는 복잡한 제한조건을 이미 완벽하게 만족하는 무수히 많은 매핑경로를 트레이닝셋으로 준다
- 제한조건에 맞는 매핑경로를 학습하게 되지만
- 제한조건의 정보가 불완전하게 디코딩되면 완벽한 매핑경로를 얻지 못한다
- 8.
- reinforcement learning : 순서대로 작동하는 일련의 과정이 데이터로 존재하지 않으면(비지도)
- 환경정보 : 매핑 경로상의 순서 및 위치정보가 없다
- 제한조건 : 컨트롤할 수 있는 제한조건이 정해진 경우, 매핑결과를 찾아낼 수 있다.
- 벡터 매핑경로의 제한조건이 매우 단순한 상황이라면(숨은 제한조건이 거의 없다면)
- 드러난 제한조건과 관련된 차원축의 수도 적다 >> 컨트롤할 차원축의 수가 줄어든다
- 단순한 액션 몇가지로 연관된 모든 차원축의 컨트롤이 가능해진다
- (이렇지 않으면 모든 차원축의 정보를 컨트롤하면서 매핑경로를 찾아야 하는게 그것은 불가능하다)
- 결과적으로 >> 매핑경로가 단순해진다
- 제한조건과 관계된 차원정보[만] 수정해서 매핑경로를 찾아나가는 것이 강화학습이다
- >> 앞으로 가는 키 : 벡터를 끊임없이 새롭게 매핑한다(랜덤성이 존재한다)
- >> 점프 : 벡터의 여러 정보가 바뀌지만, 그 중에서 제한조건에 영향을 주는 차원정보가 있다면
- # 점프키를 활용해서 그것을 벗어나도록 훈련된다
- >> 두개의 액션을 이용해서 벡터의 매핑경로가 제한조건 내에서 목표벡터에 도달하도록 훈련
- 액션이란?
- >> 벡터가 가지고 있는 (제한조건과 연관된) 차원축의 정보들을 임의로 수정할 수 있는 매핑 네트워크
- 즉, 액션은 =  주어진 벡터를 매핑하는 [네트워크]다
- 생성된 액션네트워크를 활용해서(액션 = 벡터의 일부정보를 수정하는 매핑)
- 제한조건과 연관된 정보만들 매핑하면서
- 목표벡터에 도달하는 경로를 찾아나간다
- 에이전트 : 매핑해야 하는 벡터공간의 subspace에 속한 하나의 네트워크로서
- 주어진 벡터를 이 에이전트 네트워크로 연산하면 조건을 만족하면서 매핑시킬 수 있다
- 생각은 작은 생각의 적당한 합이다 : 생각은 벡터의 매핑경로인데
- 이 말은 네트워크의 합이 생각이 될 수 있다는 뜻이다
- 액션네트워크가 가지는 특성1 : 액션 네트워크는 개념 네트워크에서 연결될 차원축을 [선택한다]
- 액션네트워크 특성2 : 선택된 차원축의 값을 자유롭게 변형시킬 수 있다
- 특성3 : 선택되는 차원축은 제한조건에 연관된 차원축이다 / 불변차원축은 건드리지 않는다
- 액션 네트워크는 개념공간의 벡터를 매핑하는데 사용된다 >> 즉 개념 자체를 다룬다
- 개념공간에서 다음 개념공간으로 매핑된 벡터를 >> 실제 데이터 벡터로 매핑한다
- 연속적인 매핑의 결론이 최종적인 매핑이 되엇다고 가정하자
- 그러나 주어지는 모든 상태에 모조리 동일한 네트워크를 써서 매핑된 결론이 완벽한 참일 수  없다
- 왜냐하면 상황에 따라 다른 매핑이 필요하고,
- 그 매핑이란게 매우 예상치 못한 매핑이 필요했다면, 지금의 네트워크로는 그러한 매핑을 할 수 없기 때문이다.
- 이게 rnn이다
- 여기서 다중액션의 필요성이 등장한다. 즉 액션이란, 이러한 매핑[네트워크]고
- 액션이 5개라는 뜻은 상황에 따라 선택할 수 있는 매핑 네트워크가 5개라는 뜻이다
- 이제 작은 액션이 모여 큰 액션이 될 수 있다는 아이디어 아래
- 처음에는 300차원 중에서 10개의 차원축만을 컨트롤 하는 액션네트워크가 5개 있다고 한다면
- 액션을 활용할 수록, 300차원중에 200개의 차원축을 컨트롤 하는 액션 네트워크가 5개 존재해야 한다
- 최초의 액션 네트워크가 5개의 노드를 선택해서 파라미터를 설정하고 컨트롤했다면
- 다음다음다음 액션 네트워크는 이제 각 액션네트워크끼리 종합하는 세로운 네트워크를 형성하고
- 그 네트워크가 주어진 차원정보를 한꺼번에 여러개 컨트롤할 수 있어야 하고
- 각 차원축을 조절하는 액션네트워크는 당연히 겹칠 수 있다
- 이것이 바로 종합적인 생각의 정의다
- 그러나 액션에 따라 어떤 차원을 증가하기도, 감소하기도 하기 떄문에, 두 액션의 컨트롤차원을 하나로 더하는데 문제가 있다 : 미해결!
- 불변차원을 늘리고 컨트롤할필요가 없는 차원을 제외한는 액션생성방법으로 바꾸자! : 해결!
- 실제로 언어와 이미지개념벡터로 다음 벡터를 매핑하는 방법을 생각해 보면 다음과 같다
- 언어공간의 매핑정보 : 액션이 아니라 rnn으로 학습된 네트워크를 이미 가지고 있다 : 수많은 숨은 조건을 이미 간접학습한 네트워크
- 이미지공간의 매핑정보 : rnn으로 이미지의 연속성이나 물리적 법칙의 수많은 숨은조건들을 이미 간접학습한 네트워크
- 둘 다 지금은 rnn때문에 단일 매핑 액션 네트워크를 가지고 있다
- 일단 이것부터 바꾸자
- 트레이닝셋을 가지고 rnn을 써서 액션을 생성할 때,
- 벡터의 노드를 제한해서 던져보자 >> 제한된 정보를 다음 벡터로 최대한 매핑하기 위해 특정한 정보를 다루는 액션이 생겨날 수 있다
- 서로 다르게 벡터의 개념차원정보를 제한해서 >> 각각의 개념정보에 대해 다루는 액션벡터를 다르게 생성한다
- 이제 생성된 5개의 액션네트워크를 가지고 전체 트레이닝셋을 강화학습하면
-  액션의 배열정보만 알게될 뿐, 액션 네트워크 자체는 그대로다
- 강화학습으로 얻어진 각 매핑 시퀀스를
- rnn에 넣으면 인해 각 매핑마다 코스트함수가 생기는데, 이 코스트함수의 총합을 감소시켜서 이용해서 액션을 1회 업데이트
- 반복시행한다
- 노드에 관한 생각을 적어보면 다음과 같다.
- 우선 어떤 [단일]액션이 트레이닝된다는 뜻은, 그 액션 네트워크가 다루는 자체 내부적으로는 매핑할때 노드끼리의 내부충돌이 없다는 뜻이다
- 그러나, 두 액션을 마음대로 결합할 수는 없다. 왜나하면
- 두 액션 네트워크가 만약 공유되는 노드(차원정보)를 가지고 있다면 두 액션은 독립적으로 작용할 수 없기 때문이다.
- 왜냐하면, 어떤 액션네트워크가 노드1을 감소시켜 매핑하는데
- 어떤 액션네트워크가 노드1을 증가시켜 매핑한다면
- 두 네트워크는 공존할 수 없기 때문이다
- 따라서 두 네트워크가 독립적일 수 있도록, 트레이닝된 액션네트워크의 노드와 파라미터는 조합될 수 없다.
- 액션끼리 더하는게 아니라
- 액션으로 인해 판단된 관계없는 노드를 제거하고 새로운 노드를 추가하는 방법을 쓰자
- 하나의 문제를 해결하기 위해 필요한 숨은 제한조건을 모두 찾아 고정하고
- 해결에 필요한 차원정보를 최소한으로 제한해서 결정한 다음
- 그 차원정보를 해결하는 n개의 액션을 생성해서
- 매핑으로 문제를 해결하는 방식으로 해보자
- 다음과 같은 방법을 제안한다
- 5개의 초기액션으로 모든 매핑을 성공적으로 마친 경우, 코스트 함수가 제시되고 액센네트워크는 업데이트된다
- 이 때, 각 액션으로 인해 벡터가 매핑되는데, 벡터의 매핑결과, 값이 거의 변화하지 않는 벡터의 차원정보가 있을 것이다
- 이 불변차원의 정보를 어케 찾는가 하면 >> 어텐션을 쓴다
- 어텐션 파라미터를 써서 주목되지 않는 노드를 알아내고 확률적으로 매번 다음 샘플링때 선택될 확률이 감소하도록 만들자
- 어텐션 파라미터가 주목한 노드는 다음 샘플링때 선택될 확률이 증가하도록 한다
- 불변차원정보는 각 액션마다 다르게 선택하고 버린다
- 왜냐하면 어떤 액션에서는 다른 액션에서 불변으로 생각하는 정보가 불변이 아닐 가능성이 있기 때문이다
- 이렇게 하면 액션의 수는 일정하게 유지한 채로, 각각의 독립성을 유지하면서, 벡터를 최선으로 매핑하는 액션을 얻게된다
- 주의사항 : 한 액션네트워크에서 제외된 노드는 다른 액션네트워크 노드가 될 수 있다!
- 액션의 수가 늘어나면 강화학습이 어려워 지고, 액션의 수가 줄어들면 매핑의 완전성이 감소한다, 둘의 적당한 중간지점을 취해주는 것이 포인트다
- 종합개념벡터의 매핑
- 각 차원공간에 존재하는 5개의 학습된 액션이 존재한다고 가정하자
- 현재 주어진 액션은 15개다
- 만약 3공간의 벡터를 하나로 합성한 경우, 액션 역시 합성해야 한다
- 3 공간의 차원축을 연결했기 때문에
- 각 액션네트워크 역시 연결할 수 있다?
- rnn이란 액션추출기다
- rnn은 각 매핑마다 하나의 네트워크를 써서 다음 벡터를 매핑한다
- 따라서 rnn은 각 매핑마다 하나의 액션을 쓰는데, 매핑에 100번이라면 네트워크가 100번 존재한다
- 그런데 rnn의 네트워크 구조를 잘 생각해보면, 모든 매핑에서 사용된 w와 b가 모두 동일하다
- 이 말의 의미는, 결국 rnn네트워크의 구조는 단 하나의 액션을 사용하는 강화학습이라는 뜻이다
- rnn이 강화학습과 다른점은, rnn은 결과를 미리 제시해서 하나의 액션 자체를 정의하고 학습하지만
- 강화학습은 결과가 미리 제시되지 않기 뗴문에, 단순한 액션을 미리 정의하고 매핑을 찾아간다는 것이다
- 따라서 모델을 생성할 때 생각할 단순한 조건은 다음과 같다
- 결과가 미리 제시된 경우(트레이닝셋이 있는 경우) : 적당한 하나의 액션을 추출할 수 있다
- 결과를 미리 알 수 없는 경우(트레이닝 셋이 없다) : 적당한 몇개의 액션을 미리 제시하고 시작해야 한다
- rnn의 모델이란 결국 다음과 같다
- 하나의 개념공간 내의 벡터끼리 숨은 조건을 고려해서 만들어진 트레이닝셋의 매핑경로를 보고
- 그 모든 매핑경로를 포함할 수 있는 하나의 액션을 트레이닝으로 생성한 것이다
- 이 액션을 여러개를 생성하고, 각 매핑마다 적당한 액션을 선택할 수 있게 하는것이 내가 생각하는 아이디어다
- 또, rnn은 주어진 벡터의 모든 차원을 전부 다 조절하는 액션벡터를 1개 생성했다
- 내가 생각하는 방법은, 주어진 벡터의 일부 sub차원만 조절하는 다중액션벡터를 10개 생성하고
- 각 매핑마다 맞는 액션을 취사선택할 수 있도록 변형하는 것이다
- rnn의 단일액션네트워크가 업데이트되는 방법을 응용해보자
- 각 매핑마다 발생하는 코스트함수를 모조리 더해서, 그것을 하나의 코스트함수로 보고 트레이닝한다
- 마찬가지 방법을 응용하면 다음과 같은 방법이 제안된다
- 랜덤한 다중액션네트워크를 일단 생성한다
- 조절할 노드와 파라미터 모드 랜덤이다
- 이제 이 네트워크가 알맞은 매핑을 할 수 있도록 인풋을 주고 아웃풋을 결정해 주면 네트워크는 훈련된다
- 인풋은 매핑을 원하는 벡터가 된다
- 아웃풋은 매핑결과다
- 매핑결결과가 옳은지 어떻게 판단할까?
- 방법1 : rnn과 같은 방법으로 한다. 즉 트레이닝셋을 제시하고,
- 각 매핑에서 액션을 선택해서 나온 코스트를 모두 더한 후, 액션네트워크를 업데이트한다
- 방법2 : 트레이닝셋이 없는 경우 감별자가 트레이닝셋 대신에 옳은지 그른지 판단하면 좋겠다
- 감별자는 무엇을 판단하는가?
- 매핑된 벡터가 내가 원하는 매핑벡터인지 판단하면 되는데
- 현재 매핑이 내가 원하는 매핑인지 알 방법이 없다. 그러나!
- 이런 매핑이 연속적으로 일어난 결과매핑벡터가 내가 원하는 게임의 목표라면, 리워드를 주기 떄문에
- 바로 이 리워드 = 코스트함수로 해서, 코스트함수를 미분하는 방식으로 각 매핑에서 사용되었던 액센네트워크의 파라미터를 업데이트한다
- attention을 이용해서, 각 매핑에 주목되었던 모든 노드를 기록해 두어야 한다
- 이 기록을 활용해서 어떤 노드가 주로 활용되는지 기록한 후에
- 다음 액션에 전승되도록 한 후에 액션네트워크가 컨트롤하는 노드의 수를 증가시킨다
- 지금 잘 하고 있는건지 판단하는 감별자를 만들고
- 이 감별자는 다른사람의 플레이나 셀프플레이를 보고 만들어진 감별자로
- 결과가 안나와도 중간중간에 각 개념요소별로 잘하는지아닌지 중간리워드를많이 제시한다
- 코스트함수를 음수로 만드는것이 매핑네트워크의 목표가 되도록 가가오하학습
- 결국 강화학습의 리워느 = gan의 감별자 코스트함수의 단순화된 형태
- 9.
- 정리하면 다음과 같다
- 우선 스타크래프트 화면이 존재한다
- 주어지는 제한조건이 매우 단순하다
- 제한조건1 : 목표 : 게임에서 승리한다
- 제한조건2 : 패배하지 않는다
- 그러나, 실제로 플레이를 시작하면, 이보다 훨씬 더 많은 [숨은] 제한조건이 존재한다
- 그러나 제한조건이 무엇인지 아직 모르는 상태이기 때문에
- 벡터의 매핑경로가 무한대에 가깝고, 따라서 결과벡터에 도달하기 어렵게 된다

9.
- >> 숨은 제한조건을 모두 고려하는 매핑경로 탐색이란 있을 수 없다
- >> 매핑경로 제한방법1
- 인간에 의헤 제한조건을 이미 고려해서 일어난 대강의 매핑경로를 rnn으로 학습한다
- 숨은 제한조건을 간접적으로 학습
- >> 매핑경로 제한방법2
- 3가지 차원공간에서 온 종합벡터 : 벡터의 매핑경로에 많은 제한조건이 추가된다
- 위의 2가지 방법 모두 매핑경로를 단순하게 만든다
- 만약 강화학습을 먼저 선택한 경우를 고려한다면
- (다중차원정보벡터로) 단순한 매핑경로 상태에서 액션을 사용해서 적당한 매핑경로를 뽑아낼 수 있다
- 물론 이는 장기적으로 최적경로가 아니다
- 이제 선택된 매핑경로를 방법1로 학습한다
- 다시 방법2를 활용해서 매핑경로를 뽑는데 rnn으로 학습되어 샘플링된 매핑경로의 도움을 받자.
- 도움을 받는 방법은 아래와 같다.
- rnn이 제시하는 매핑경로는 최적은 아니지만 목표에 도달 가능한 매핑경로이기 떄문에
- 전체적인 매핑경로는 비효율적이지만, 국소적인 매핑은 최적인 경우가 존재한다
- 따라서 rnn이 제시하는 매핑경로 정보를 벡터화한 다음에
- 이것의 [국소적인] 부분을 [attention]해서 도우미정보벡터로 변환하자
- 이제 이 도우미정보벡터를 현재 강화학습의 벡터정보에 추가한다
- 이제 이 벡터를 다음 벡터로 매핑하는 액션을 취한다
- rnn이 제시하는 국소적인 정보가 추가되어 다음 매핑이 더 확실해진다
- 만약 제4의 차원공간이 생성된다면, 이후로 일어나는 제5의 차원공간 학습에 4개의 차원공간 정보가 활용될 수 있다.
- 차춴공간과매핑네트워크의 다양성으로 인해 생각은 계속 정교해지고 다양한 방법으로 생각하게 된다.
- 차원공간을 늘리는데 순서가 필요하기 때문에, 사고를 훈련하는데 제시하는 순서가 중요하다
- 기타
- rnn 번역이 완벽하지 않는 이유는?:
- 경로매핑이 완벽하지 않고,
- 객체정보도 부족하며,
- 하나의 차원공간에서 중간개념차원공간을 거치지 않고 즉각적인 결과차원으로 매핑하기 떄문이다
- 게다가 번역할 두 언어가 존재하는 차원공간의 매핑경로네트워크 상태가 다르다, 즉 어순이 다르다
- 주어진 벡터가 고정된 자리에 존재하거나 거의 변화하지 않는 경우에
- 처벌함수를 생성해서 벡터가 끊임없이 새로운 자리로 매핑될 수 있도록
- 설계한다 >> 만약 마우스를 클릭해도 주어진 환경이미지의
- 벡터에 거의 변화가 없는 경우인데 제한조건에 걸리지 않는다면?
- >> 이 차원축은 환경정보일 가능성이 크다
- 이미지캡셔닝도 원리는 동일하다
- 이미지 >> 이미지의 개념을 담은 공간벡터로 변환 >> 이미지개념을 종합하면 cat이라는 단어가 되도록 매핑
- 벡터의 매핑이 선형변환이 될 수 없기 때문에 복잡한 비선형변환를 쓰는데
- 만약 2벡터의 위치를 이미 알고 있다면, 단순하게 두 벡터를 뺴는 연산이 두 벡터의 매핑이다
- 다만 이는 같은 개념공간 내에서만 연산할 수 있는 방법이 된다
- 항원 항체 반응을 본따서 생각의 구조를 만들자
- 커다란 항체가 있고 항체의 각 수용부분에 알맞은 항원을 끼우는 방법도 좋고
- 사슬방법으로 여러개의 레고블럭을 연결하는 방법도 좋다
- 하나의 블럭에 여러개의 수용체 형태가 존재하고, 각 수용체에 맞는 다른 블럭이 결합되면서
- 사슬이 아닌 구조체 형태로 구조가 만들어 진다
- 인풋 블럭이 결합되면, 반대작용으로 아웃풋 블럭 수용체를 생성한다
- 수용체의 형태는 미분기하에 의해 변형될 수 있다

주어진 문제를 작게 분할한다
분할한 문제의 변수들을 객체에 할당한다

공간에 객체가 있을 경우
객체간의 움직임이 자유롭다고 가정하고

모션 하나하나가 의미하는 중간 언어를 학습

언어 명령을 통한 객체간의 움직임을 통한 결과와 실제 모델 결과의 공간상에서의 일치도 차이가 최소화 되는 방식으로 모션을 설정

반복?

베이지안 방법을 활용해서 세계의 상품들간의 가격 변동을 변수로 하는 알고리즘을 만들어 보자




1 + 1 = 2라는 것을 인식시켜 보자

수를 학습한다
더하기를 학습한다

공간상의 객체상태 변화를 인식하도록 만든다 >> 어떻게?
객체의 


수를 학습한다 > 공간상의 객체의 개수 = 수 라고 학습하도록 만든다
더하기를 학습한다 > 공간상의 객체의 개수가 증가하면 더하기가 된다고 학습하도록 만든다

큰 수의 더하기 > 객체를 직접 생성하는 방식으로 더하기를 계산하는 것
더하기를 학습한 후 계산방식을 생성해서 더하기를 계산하는 것
각각의 비용함수 중 최소인 방식으로 선택한다?



1 + 1 = 2 이해시키기

외부 세상을 본다
외부 세상에서
개념 >> 객체화 된다
그래픽 >> 입혀진다

세계에 대한 정보가 단위시간단위로 3d화 되어서 각각의 개별화된 사진처럼 분할되고
시간의 흐름에 따라 그 전에 사진은 개념화 >> 희미
현재 보는 사진 >> 개념화 + 선명

사진의 모든 부분을 다 개념화하지 않는다
시야각의 중심에 있는 것, 포커스를 맞추고자 의지를 가지는 대상에 개념화가 일어난다

시야에 들어오는 대상을 픽셀화 해서 >> 재조립 하는 과정을 거치는 것이 머신러닝의 비쥬얼 분야이다
재조립 과정을 2가지로 나누어서 생각하자
재조립1 : 픽셀단위로 구분하고 재조합하면서 사물을 구분한다
재조립2 : 색깔과 그래픽을 덧입힌다

사물을 구분하면 >> 단순화 작업을 거쳐서 데이터의 정보량을 최소화 한 상태(물리엔진 활용)로 바꾸어서 스냅샷을 저장한다

스냅샷은 불연속이지만, 세계의 구현은 연속이라고 가정하면
그 전에 존재한 스냅샷의 정보 중에서 변화한 부분만 재활용하고, 변하지 않은 부분은 전의 시간데이터를 복사해서 활용해서 메모리를 줄인다
개념의 위치는 어느정도 유연성을 가진다/ 라기보다는 개념끼리의 관계 = 개념끼리의 상태 를 저장하는것이 낫다

머신러닝은 분류다
무엇을 분류할 것인가?
주어진 스냅샷과 그 전 시간대의 스냅샷을
큰 프레임 단위로 자르고
변화정도가 큰 프레임이 존재하면 그 부분만 개념을 재구성한다
이 때 변화를 알아내기 위해서 분류기가 필요하다

변화를 관측하면 이 변화를 개념으로 저장한다
이 때 개념스냅샷의 변화 상태를 하나의 개념으로 이해한다
하나의 개념으로 이해하고자 할 때에는 여러개의 스냅샷 변화를 관측해야 한다
이떄 관측하는 스냅샷의 개수가 적은 것이 효율이 높을 것이다
어떻게 그럴 수 있는가? 코스트함수는 여전히 필요한가?

ㄴㄴㄴㄴ이게아니다 코스트함수가 아니다
여러가지 스냅샷의 변화를 관측할 때
그 관측한 스냅샷의 단순화 버전스냅샷이 모두 일치하면 그 샷의 이동을 하나의 개념으로 데이터화하고 저장한다

즉 객체화된 결과가 다수 일치하면, 일치해서 정립된 단 하나의 스냅샷, 또는 스냅샷의 짧은 변화(프레임2개, 많으면 3개)를
하나의 개념으로 이해한다.
요기서 중요한 것은, 무엇을 단순화하는가? 어떻게 단순화하는가? 라고 할 수 있는데
지금 떠오르는 생각으로는
포커스를 맞추는 대상(이를 변화하는 대상으로 정하자)을 단순화 해야하고 / 단순화하는 대상은 객체 + 객체관의 관계성 + 스냅샷의 변화 이다

동적개념을 정의할때는 스냅샷의 변화를 개념화하고
정적개념을 정의할때는 스냅샷 자체를 개념화하면된다
우리는 5감을 가지고 있어서 스냅샷의 변화를 5가지로 이해하는것이 가능하다
따라서 변화라는 개념은 매우 복합적이며, 5가지 감각의 적당한 조화로 상태를 이해한다
다시말하자면 feature가 많아서, 스냅샷의 변화를 개념화 하는데 언더피팅이 안된다는 것이다
그러나 시각으로만 피팅하면 언더피팅이 일어난다
따라서 물리엔진으로 5감뿐만 아니라 적당히 여러가지 feature를 설정하는 것이 유리한데
물리적 속성인 질량, 탄성, 색깔 등을 활용하면 될 것이다.



단 이때 개념화되는스냅샷은 실제 복잡한 세계를 매우 단순화시킨것으로, 그 단순화된 결과가 일치하는 것들을 개념화한다

1. 어떻게 단순화할까? : 물리엔진 이용?
2. 단순화된 결과가 일치하는지 어케 아는가? : 공간상태이용?

개념을 두가지로 나누어 생각한다
129 + 333 을 계산할 때
129 / 333과 +를 서로 다른 방법으로 인식하는 것이다
+는 개념스냅샷을 활용하여 1 + 1의 경우를 생각한다면 이해하고 있는 것이다
실제 객체를 100개이상 부르는 것이 비효율적이다
따라서 이러한 비효율 상황이 발생하거나, 프로그래머가 미리 제어해둔 객체생성한정범위를 넘어서게 되면
자동적으로 이러한 스냅샷을 기호화할 수 있도록 주어진 객체를 추상화하고 기호화하는 작업을 수행한다
하나의 스냅샷은 정적인 것을 기호화하고
스냅샷의 변화를 하나의 기호로 만든다
기호할당이 완료되면 
어떠한 상황이 주어질 때
2가지 방법으로 모두 생각할 수 잇도록 한다

복잡한 상황을 단순화하는 것에 대해서도 고려해보자 이것은 어케 일어나는가