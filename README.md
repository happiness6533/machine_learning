## 작업 리스트
1. octave 코드를 python 코드로 변환중
2. 텐서플로우2 적용중 변환중

## 머신러닝 정리중
0. 벡터는 위치와 방향을 가지고 있다
위치라는 개념을 없애서 모든 도메인을 통합하자
위치가 있기 때문에 모든 도메인을 하나로 만들 수 없는 것임

하나의 구조체와 다른 구조체가 동시에 존재할 때 어떤 연관성이 존재하는가?만 고려하고, 위치는 생각하지 않는다

1. 풀이 방법: 정보의 추상화
    - 확률대수관점 MLE
    - 기하관점 벡터공간 변환
      
2. 지도 학습: 데이터 분류 trained by x and y
    - 고전: svm hmm naive-bayes regression
    - 현대
        - 일반 데이터: dnn
        - 지역성 데이터: cnn
            - 시각 데이터: yolo style-transfer resnet u-net
        - 순서성 데이터: rnn imbedding
            - 자연어 데이터: transformer bert gpt
            - 청각 데이터: tacotron2 wave-net

3. 비지도 학습: 데이터 분류 trained by x / find latent z / 샘플링
    - 데이터를 바로 이진분류: gaussian anomaly
    - 데이터를 바로 다중분류: k-means > gm > em > vi
    - find z: pca kpca t-sne encoder > x를 넣으면 latent z를 알려주는 확률분포를 얻는다
    - 데이터로부터 find z + 데이터를 분류 from z: ae
    - 데이터로부터 find z + 데이터를 분류 from x and z: vae
    - 데이터를 분류 from z and 데이터를 확인 from x: gan

5. 강화학습: 셀프 판단 or 행동
    - 정책기반: 폴리시이터 q러닝 reinforce
    - 가치기반: 밸류이터 sarsa
    - 가치정책기반: deepsarsa dqn a2c

6. 텐서플로우를 활용해서 모델을 구현하고 현실의 문제를 해결

## 지도학습 알고리즘
1. 머신러닝1 : 선형 회귀 : 데이터의 "진행"패턴이 존재하는 상황에서 다음 데이터의 값을 예측하는 함수를 만들고 싶다

2. 피쳐 스케일링
    - 가설함수 형태 : 무리, 선형, 다항함수로 적당히 조합해서 가정
    - 차이값 제곱의 합 / 2m =  코스트 함수(오버피팅도 동시에 해결하자)
    - 코스트 함수가 최소가 되는 세타를 구한다
        -코스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
        -노말 이퀘이젼
        -코스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
    - 세타로 함수확정 = 결과의 분포와 비슷한 함수

3. 머신러닝2 : 로지스틱 회귀 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
    - (매우 단순한 분류만 가능하다)(데이터의 분포패턴 = 벡터)

4. 피쳐 스케일링
    - 가설함수 형태 : 무리, 선형, 다항함수로 적당히 조합해서 가정
    - 로그와 시그모이드(가설함수값)을 잘 버무려서 / m = 코스트 함수(오버피팅도 동시에 해결하자)
    - 코스트 함수가 최소가 되는 세타를 구한다
        - 스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
        - 스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
    - 세타로 함수확정 = 분류를 결정하는 디시젼 바운더리

머신러닝3 : 뉴럴 네트워크 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
(변수끼리 관계에 의한 매우 복잡한 분류도 가능하다)(데이터의 분포패턴 = 벡터)
(아웃풋 레이어의 개수에 따라 멀티분류가 자체적으로 가능하다)

로지스틱 회귀를 사용한 단일 레이어로는 분류가 불가능한 데이터 패턴이 존재한다
로지스틱 회귀의 분류값을 뉴런의 활성유뮤로 생각하자
로지스틱 회귀를 사용한 뉴런의 다중 레이어를 이용하면 단일 레이어로 분류가 불가능한 데이터 패턴도 분류할 수 있다
최적화 함수를 잘 쓰자(옵션 정의 / 이니셜 세타 정의 / 코스트펑션에 코스트함수와 미분계수 정의 / 최적화세타 = fminunc(코스트펑션/이니셜세타/옵션))

머신러닝4 : 실습 : 스팸 분류기를 만들어 보자
    - 이메일 데이터를 활용해서 이메일에 자주 등장하는 단어사전을 생성한다
    - 각각의 이메일 단어 분포패턴 기록 >> 단어의 등장유무에 따라 0 / 1을 부여해서 각각의 이메일을 벡터화
    -  : 이메일을 벡터화할 때, 단어의 분포패턴 이외에 이메일의 헤더 분포패턴도 활용하면 좋다
    -  : 단어사전을 만들 때, 원형이 같은 단어는 같은 단어로 분류하는 것이 효율성 측면에서 좋다
    -  : 단어사전을 만들 때, 단어의 디테일한 측면(이상한 단어의 등장, 이상한 기호의 등장)을 잘 반영하면 분포패턴이 더 정확하다
    - 뉴럴네트워크로 분류한다

- 머신러닝5 : 실습 : 암 분류기를 만들어 보자
- 
- 말짱하다 99% / 암이다 1% 의 데이터가 있다고 하자
-  : 모든 사람이 말짱하다 = 99% 맞았다
-  : 머신러닝 열심히 = 99% 맞았다
-  : 이렇게 되면 머신러닝이 의미가 없다
- 
- 프리시젼 = 암으로 분류가 진짜였다 / 암으로 분류
- 리콜 = 암으로 분류가 진짜였다 / 원래 암
- 
- 모든 사람이 말짱하다
-  : 프리시젼 = 값이 정의되지 않는다(모두 말짱하다고 했으므로 암으로 분류한 적이 없다)
-  : 리콜 = 0(극소수이지만 원래 암인 사람을 / 암이라고 맞춘 적이 없다)
-  : 이 알고리즘은 구라임을 알 수 있더
-  : 따라서 프리시젼과 리콜은 데이터의 1%에 해당하는 분류결과로 정의
- 
- 1. 시그모이드(가설함수) = 확률 : 확률이 얼마 이상이면 y = 1이라고 예측하는 것이 좋을까?
- 
- 예측확률 > 0.9 : 1이라고 생각한다 = 예측한 1이 / 실제 1이었을 확률이 높다 = 프리시젼이 높다 
- 예측확률 < 0.9 : 0이라고 생각한다 = 실제 1인데 / 1이라고 예측할 확률이 낮다 = 리콜이 낮다
- 
- 예측확률 > 0.1 : 1이라고 생각한다 = 예측한 1이 / 실제 1이었을 확률이 낮다 = 프리시젼이 낮다 
- 예측확률 < 0.1 : 0이라고 생각한다 = 실제 1인데 / 1이라고 예측할 확률이 높다 = 리콜이 높다
- 
- 그렇다면 기준값을 어떻게 정할까?
- 프리시젼과 리콜의 비율로 결정한다
- 즉 (0 < 프리시젼과 리콜의 조화평균 < 1)을 가장 크게 만드는 값을 선택한다
- 
- 2. a / b / c 멀티분류
-  : a에 대해 분류해서 얻은 확률함수값 / b에대해 분류해서 얻은 확률함수값 / 1 - (앞의 두 확률의 합)
-  : 새로운 데이터가 주어지면 세 값을 모두 계산하고 확률함수값이 max인 클래스를 선택한다
- 
- 3. 이니셜 세타
-  : 0이 나오면 망하는 경우가 많으니까 0-1사이의 랜덤값으로 이루어진 행렬을 만들고
-  : 곱하기 2엡실론 마이너스 엡실론
-  : 범위를 -엡 +엡 사이로 만든다
-  
- 
- - 내 그래디언트 디센트가 맞는지 틀렸는지 불안하면 그래디언트 체킹을 하자
-  : 매우 작은 입실론값에 대해 평균변화율을 계산해서 미분계수와 거의 일치하면 맞는것으로 인정
-  : 백프로파게이션을 한 번정도 시행하고 일치여부를 확인하며, 거의 일치하면 체킹을 끄고 본격적으로 전파왕복을 시작해야 한다
- 
- - 오버피팅이란?
-  : 트레이닝셋으로는 잘 맞았는데, 트레이닝 셋에만 너무나도 잘 맞는 가설함수를 선택한 나머지, 실시간 데이터에 대해 예측이 심하게 빗나가는 현상을 말한다
- 
- - 오버피팅 해결1
-  : 피쳐의 개수와 식의 복잡도를 줄이자
- 
- - 오버피팅 해결2
-  : 선형 회귀 / 로지스틱 회귀 / 뉴럴 네트워크 모두 코스트 함수에 (바이어스 세타를 제외한 모든 세타의 제곱의 합 * 람다 / 2m)을 더한다
-  : 미분하면 (바이어스 세타를 제외한 모든 세타 * 람다 / m)이다
-  : 적당히 큰 람다라면 코스트 함수가 최소값을 가지기 위해 자동으로 세타를 작게 만들게 된다 : 그 세타에 붙어있는 피쳐의 영향력이 줄어든다 : 가설함수의 차수가 낮아지는 효과가 생긴다 : 오버피팅 해결!
- 
- - 오버피팅 해결3
-  : 노말이퀘이젼 괄호안에 (람다 * 단위행렬(바이어스 부분은 0))을 더하고 계산한다
- 
- - 진단1 : 내가 만든 머신러닝 프로그램의 가설함수가 오버피팅을 유발하는 함수인지? 상태를 진단하고 올바른 가설함수 형태 찾아보기! (차수와 람다를 조절해서 피쳐의 개수에 영향을 줘보자!)
-  : 트레이닝셋 60% / 평가셋 20% / 테스트셋 20%
- 
-  : 10개의 람다를 만들고 각각의 람다에 대해 10개의 다양한 가설함수를 만들어서(100개를 만들어라ㄷㄷ)
-  
-  : 첫번째 람다에 대한 10개의 가설함수로 트레이닝셋에 적용해서 각각의 경우에 대해 코스트함수가 최소가 되는 세타 10개를 구한다(일단 트레이닝)
-   : (이 때는 너무 당연하게 코스트 함수에 람다를 쓴다!)
-   : (트레이닝셋에만 최적화된 세타들ㅠ)
-   : (당연히 가설함수의 차수가 높아짐에 따라 코스트 함수가 작아진다)
- 
-  : 10개의 세타를 평가셋에 적용해서 코스트 함수가 최소가 되는 세타와 가설함수를 고른다(이제 평가)
-   : (이 때는 같은 람다로 만든 세타들 중에서 고르는 거니까, 너무 당연하게 코스트 함수에 람다를 쓰지 않는다!)
-   : (이렇게 해서 트레이닝셋에만 최적화되지 않는, 일반적으로 코스트 함수를 최소화하는 세타를 찾을 수 있다!)
-   : (당연히 가설함수의 차수가 낮은 상태에서 높은 상태로 변함에 따라 코스트 함수는 감소하다가 커진다 = 언더피팅에서 최적화상태가 되었다가 오버피팅된다)
- 
-  : 위의 과정을 간단히 정리하면 다음과 같다
-  : 트레이닝셋코스트와 평가셋코스트가 모두 높고 비슷하면 = 언더피팅 / 트레이닝셋코스트는 작은데 평가셋코스트만 높으면 = 오버피팅
- 
-  : 이렇게 100개를 반복해서 최강의 콤보(람다와 차수)를 결정한다
- 
-  : 최종 콤보를 테스트셋에 적용해서 코스트 함수를 최소화해보고, 그래도 문제가 있다면 위의 작업을 다시 해보자
- 
- - 평가셋과 테스트셋의 코스트함수 특이하게 사용하기
-  : 로지스틱 회귀는 테스트셋으로 코스트 함수를 계산할 때
-   : 잘못된 결과(가설함수는 0보다 큰데 y는 0이라던지 / 가설함수는 0보다 작은데 y는 1이라던지) = 1
-   : 옳은 결과 = 0
-   : 위의 두 결과를 다 더해서 m으로 나누는 방법으로도 계산할 수 있다
- 
- - 진단2 : 내가 만든 머신러닝 프로그램의 가설함수가 오버피팅을 유발하는 함수인지? 상태를 진단하고 올바른 가설함수 형태 찾아보기! (데이터를 추가하는 것이 도움이 될까?)
-  : 트레이닝셋의 개수를 1 > 100개로 증가시키면서 트레이닝셋의 코스트 함수를 계산 = 모든 데이터를 다 맞추다가 오차가 발생하기 시작 = 코스트 함수값은 0 > 50
-  : 트레이닝셋의 개수를 1 > 100개로 증가시키면서 평가셋 전체의 코스트 함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하면서 오차가 감소 = 코스트함수값은 100 > 50
- 
-  : 언더피팅 상태라면
-   : 트레이닝셋의 코스트함수를 계산 = 모든 데이터를 다 맞추다가 오차가 크게 발생하는데, 큰 오차를 줄이는데 한계가 빠르게 발생 = 코스트 함수값은 0 > 70
-   : 평가셋의 코스트함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하기는 하는데, 성능이 증가하는데 한계가 빠르게 발생 = 코스트 함수값은 100 > 70
-   : 따라서 이미 언더피팅인 상태에서는, 한계가 빠르게 발생하기 때문에, 데이터를 더 입력해도 의미가 없다
-   : 진단 1의 방법으로 차수와 람다를 조절해야 한다
- 
-  : 오버피팅 상태라면
-   : 트레이닝셋의 코스트함수를 계산 = 모든 데이터를 다 맞추다가 오차가 사알짝 발생하지만, 그 오차가 거의 없다 = 코스트 함수값은 0 > 10
-   : 평가셋의 코스트함수를 계산 = 아무 데이터도 맞추지 못하다가 성능이 증가하기는 하는데, 트레이닝셋만 너무 잘맞추고 평가셋에 대해서는 일반화가 안됨 = 코스트 함수값은 100 > 80
-   : 따라서 이미 오버피팅인 상태에서는, 위의 두 값의 차이가 크기 때문에, 차이가 감소할때까지 데이터를 더 입력하는 것이 크게 도움이 된다
-   : 진단 1의 방법으로 차수와 람다를 조절해야 한다 + 데이터를 더 투입한다
- 
- - 진단 후 해결방법 총정리
-  : 오버피팅이라면? : 차수 감소 / 람다 증가 / 더 많은 트레이닝셋 투입(많은 데이터는 오버피팅을 상쇄한다) / 뉴럴 네트워크라면 레이어와 파라미터 개수 감소(람다를 활용하는게 더 낫더라)
-  : 언더피팅이라면? : 차수 증가 / 람다 감소 / 더 많은 트레이닝셋 투입은 무의미 / 뉴럴 네트워크라면 레이어와 파라미터 개수 증가(람다를 활용하는게 더 낫더라)
-  
-  
-  
-  머신러닝6 : 서포트 벡터 머신 : 데이터의 "분포"패턴이 존재하는 상황에서 다음 데이터의 값은 어디에 포함되는지 분류하고 싶다
- (변수끼리 관계에 의한 매우 복잡한 분류도 가능하다)(데이터의 분포패턴 = 벡터)
- 
- 피쳐 스케일링?
- 가설함수 형태 : 커널함수를 사용해서 변수를 트레이닝셋의 개수만큼 생성한다
- relu와 시그모이드(가설함수값)을 잘 버무려서 = 코스트 함수(오버피팅도 동시에 해결하자)(람다 대신 c를 쓴다)
- 코스트 함수가 최소가 되는 세타를 구한다
- -코스트 함수가 감소하도록 알파를 잘 고르고 미분계수 구해서 그래디언트 디센트 : 코스트 함수가 적당히 수렴하면 멈춘다
- -코스트함수, 미분계수만 구하고 최적화함수를 돌려버리자!
- 세타로 함수확정 = 분류를 결정하는 디시젼 바운더리
- 
- - 확률과 추정에 관한 정리1(로지스틱 회귀 + 뉴럴 네트워크)
-  : 데이터의 y = 1이라면
-  : 시그모이드(z) = h(z) = 1에 가까워야 한다
-  : cost(h(z)) = h(z)가 1에 가까울수록 작아지게 만들고 + cost(h(z))를 최소화하는 z를 찾는다(실제 y와 h(z)의 오차가 0에 가까워 지는 순간까지 z를 찾는다)
-  : 찾은 z = 디시젼 바운더리
- 
-  : 이제 새로운 데이터가 들어오면 디시젼 바운더리 z를 활용해서
-  : z를 경계로 z > 0 이면 / 시그모이드(z) = h(z) > 0.5 이면 / y = 1로 판단한다
- 
- - 커널함수로 변수 생성하기
-  : 트레이닝셋 1000개 중에서 1번 데이터
-  : 데이터의 y = 1이라면
-  : 데이터의 변수 x1, x2 >> 커널함수에 넣으면 >> 트레이닝셋의 개수인 1000개만큼의 변수 x1 - x1000 생성
-  : 각 변수는 현재 데이터와의 비슷한 정도를 0 - 1 사이의 숫자로 나타낸다
-  : 비슷할수록 1 / 다를수록 0이다
-  : 비슷하면 값을 증가시키고, 다르면 값을 감소시켜서 모두 더한 합을 z라 한다
-  : h(z)가 1에 가까워야 한다
- 
- - 확률과 추정에 관한 정리2(서포트 벡터 머신)
-  : 데이터의 y = 1이라면
-  : 시그모이드(z) = h(z) = 1에 가까워야 한다
-  : cost(h(z)) = h(z)가 0.8에 가까울수록 작아지게 만들고 + 0.8 이상이면 0이 되도록 만들고 + cost(h(z))를 최소화하는 z를 찾는다(실제 y와 h(z)의 오차가 0.2에 가까워 지는 순간까지 z를 찾는다)
-  : 찾은 z = 디시젼바운더리
- 
-  : 이제 새로운 데이터가 들어오면 디시젼 바운더리 z를 활용해서
-  : z를 경계로 z > 0이면 / 시그모이드(z) = h(z) > 0.5 이면 / y = 1로 판단한다
- 
- - 서포트 벡터 머신의 코스트함수 최소화 과정
-  : cost(h(z)) 최소화
- 
-  : cost(h(z)) = 코스트함수부분 + 오버피팅부분
-   : 코스트함수부분 최소화 = 0으로 최소화
-   : h(z) > 0.8이 되도록 z를 조절한다
-   : z > 1이 되도록 z가 조절한다
-   : z = 세타와 x의 내적이다
-   : z = 세타길이 * x길이 * 코싸인 > 1이 되도록 z가 조절한다(x길이는 일정하다)
-  : 오버피팅부분 최소화
-   : 세타길이 최소화
- 
-  : 위의 두 결론을 종합하면, 코스트함수부분을 0으로 최소화하면서, 오버피팅부분 최소화하려면, 코싸인값이 커져야 한다 = 코싸인이 1이다 = 각이 0 = 세타와 x는 거의 같은 방향의 벡터다
-  : 세타는 디시젼 바운더리 z의 법선벡터이기 때문에, 이 상황을 그림으로 생각해 보면, 법선벡터가 데이터를 가르키는 벡터와 거의 평행하다는 뜻이다
-  : 디시젼 바운더리는 자연스럽게 두 데이터 집단의 가운데에 마진을 가지고 존재하게 된다



## 비지도학습 알고리즘
1. pca = ae의 특수한 버전임이 증명되어 있다
    - 람다를 이용한 오버피팅 문제가 해결이 안되는 경우
    - 데이터가 가지고 있는 feature의 개수(차원의 수)를 축소할 수 있다
    - 
    - 우선 가지고 있는 트레이닝셋 데이터를 피쳐스케일링하고, 평균을 0으로 맞춘다
    - 이제 목표하는 축소된 차원의 수를 k라고 하자
    - 
    - k = 1`
    - svd함수로 데이터의 공분산행렬 생성 
    - 차원축소가 완료된 데이터 = 공분산행렬'(k번째 열까지) * 데이터
    - 차원축소된 데이터의 복구 = 공분산행렬(k번째 열까지) * 데이터(대칭행렬의 역행렬은 transpsse와 같기 때문)
    - s행렬을 활용해서 정보의 retain정도를 확인, 99%보다 낮으면 k를 증가시켜서 반복
    - 
    - 트레이닝셋의 데이터차원을 축소하는 k와 공분산행렬을 찾았다면
    - 이제 그 파라미터를 사용해서 cv셋과 t셋에 사용한다


## 강화학습 알고리즘
1. 주어진 문제를 mdp로 구성
    - mdp 핵심 구성요소
        - 상태: 유한 / 무한
        - 행동: 상태를 변수로 가지고 행동을 결과값으로 가지는 pmf/pdf 를 정책이라고 한다
        - 보상

    - mdp 하이퍼 파라미터
        - 상태변환확률 : 행동에 의해 어떤 상태에서 다른 상태로 변화할 확률들의 집합
        - 감가율

    - mdp 풀이
        - 어떤 타임 t에서의 어떤 상태 s에서, 현재 정책에 따른, 감가율을 고려한, 미래에 얻을 수 있는 모든 보상의 합 = gt
        - 어떤 타임 t에서의 어떤 상태 s에서, 현재 정책에 따른, 감가율을 고려한, 미래에 얻을 수 있는 모든 보상의 합의 기대값 = v(s)
        - 현재 v(s)에 의한 정책에 따른 semi-real v(s) 얻기 + 얻은 v(s)로 정책 업데이트 >> 반복 = optimal v(s) + optimal policy(s) 획득
        
2. mdp를 풀이하기 위한 value and policy 업데이트 방법
    - v(s) 업데이트 방법1
        - 모든 상태 s가 유한한 경우, 모든 v(s)를 직접 계산할 수 있다
        - gt의 정의로부터 아래의 식을 유도한다
        - v(s) = 시그마(a 확률 * (reward + 감가율 * v(s'))
        - 위의 수식에서, 현재 상태 s와 어떤 행동 a에 대해 얻어지는 상태 s'의 관계는
        - s에서 a를 해서 s'이 됨
        - 과 같으므로, 우리는 v(s')을 s와 a에 대한 함수로도 생각할 수 있다. 이러한 함수를 q 함수라고 한다면 다음이 성립한다
        - v(s') = q(s, a)
        - 따라서 상태 s는 가능한 행동 a들에 대해 여러 q(s, a)를 가진다
        - 따라서 위의 식을 다시 쓰면 아래와 같다
        - v(s) = 시그마(행동 확률 * (reward + 감가율 * q(s, a))
        - 여기까지 따라왔다면, 위의 수식을 조금 더 일반화시키면 아래와 같음을 이해할 수 있다
        - q(s, a) = 시그마(행동 확률 * (reward + 감가율 * q(s', a'))
        - 결론적으로 아래의 두 수식을 활용한다
        - v(s) = 시그마(행동 확률 * (reward + 감가율 * v(s')): 정책 이터레이션, 가치 이터레이션
        - q(s, a) = 시그마(행동 확률 * (reward + 감가율 * q(s', a')): sarsa, q러닝, deepSarsa, dqn

    - v(s) 업데이트 방법2
        - 모든 상태 s가 무한한 경우: 모든 v(s)를 직접 계산할 수 없다: mc(몬테카를로) 방법을 사용한다
        - 에피소드1: 지나온 경로를 기록하고, 경로에 따른 gt를 각 스테이트에 기록하자
        - 에피소드2: 지나온 경로를 기록하고, 경로에 따른 gt를 각 스테이트에 기록하자
        - 에피소드 반복
        - 이제 기록된 모든 스테이트에 따른 gt 기록들의 평균을 구하자 = v(s)
        - 이 평균은 지금 정책에 따라 얻어지는 참 가치함수에 수렴한다
        - 그런데 이렇게 하려면, 에피소드 1000번동안 있었던 모든 걸 다 기록해야 되는데, 이건 메모리가 많이 든다
        - 따라서 테크닉을 사용, 임의의 n번째 에피소드가 종료되면 즉시 v(s)를 업데이트 해도
        - 1000번의 에피소드를 마치고 v(s)를 업데이트한 결과와 같도록 수식을 구성한다
        - 업데이트할 v(s) = 기존의 v(s) + 1/n * (현재 gt - 기존의 v(s))

    - v(s) 업데이트 방법3
        - 모든 상태 s가 무한한 경우 : 모든 v(s)를 직접 계산할 수 없다 : td(temporal difference) 방법을 사용한다
        - full 에피소드 없이, 액션에 일어날 때마다 v(s) 실시간 업데이트
        - 업데이트할 v(s) = 기존의 v(s) + learning_rate * (현재 받는 리워드[에피소드 필요ㄴㄴ] + r * v(s')[에피소드 필요ㄴㄴ] - 기존의 v(s))
        - 반복해서 현재 정책에 따른 참 가치함수값을 얻고, 정책을 업데이트하고, 이를 반복한다

    - v(s)와 정책 업데이트 방법1 - 1
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 반복 업데이트: 모든 상태의 현재 정책에 따른 real v(s)를 얻는다
        - 얻어진 real v(s)에서 그리디를 적용해서 정책 1회 업데이트
        - 위의 2과정을 반복

    - v(s)와 정책 업데이트 방법1 - 2
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 1회 업데이트: 모든 상태의 현재 정책에 따른 semi-real v(s)를 얻는다
        - 얻어진 real v(s)에서 그리디를 적용해서 정책 1회 업데이트
        - 위의 2과정을 반복

    - v(s)와 정책 업데이트 방법2
        - 모든 상태의 v(s)를 적당한 값으로 초기화하고, 반복 업데이트: 모든 상태의 현재 정책에 따른 real v(s)를 얻는다
        - 위의 업데이트에서 max v(s')을 사용하면 그리디를 적용하는 효과가 있기 때문에, 정책을 따로 업데이트 할 필요가 없다
        - 위의 과정을 반복

3. mdp 문제 해결
    - 정책 이터레이션
        - 모든 상태 s가 유한한 경우: 정책 이터레이션
        - 현재 정책에 따라 얻어지는 참 가치함수값을 구하기 위해서 v(s) 업데이트를 여러번 반복한다
        - 이렇게 반복해서 얻어진 참 가치함수를 통해 정책을 업데이트 한다
        - 업데이트 된 새로운 정책에 따라 얻어지는 참 가치함수값을 구하기 위해서 v(s) 업데이트를 여러번 반복한다
        - 이렇게 반복해서 얻어진 참 가치함수를 통해 정책을 업데이트 한다
        - 이 과정을 반복하면, optimal 정책망이 얻어진다
        - 정책망에 따른 대강의 가치함수 >> 대강의 가치함수에 의한 정책망 업데이트 를 반복해도 같은 결론으로 수렴하게 된다

    - 가치 이터레이션
        - 모든 상태 s가 유한한 경우: 가치 이터레이션
        - 가치함수를 업데이트 할 때, 평균이 아니라, 최대값을 골라서 업데이트 한다
        - 무한히 반복하면 가치함수 자체가 옵티멀 정책을 내제하게 된다

    - sarsa
        - 상태 s가 너무 많은 경우: sarsa
        - td + 가치 이터레이션
        - 업데이트할 q(s, a) = 기존의 q(s, a)
        - + learning_rate * (현재 받는 리워드[에피소드 필요ㄴㄴ] + 감가율 * max q(s', a')[에피소드 필요ㄴㄴ]- 기존의 q(s, a))
        - 다음 스테이트 s'에서 최고의 큐를 반환해주는 a'을 그리디하게 선택해서 업데이트를 먼저 하고
        - 실제 이동은 입실론 탐험이 반영되지 못한, 이미 선택된 a'에 의해 이동한다
        - 맥스를 선택하기 떄문에, 정책평가가 이미 가치함수에 반영된다
        - 정책평가 없이 반복적인 가치함수 업데이트!

    - q러닝
        - 상태 s가 너무 많은 경우: q러닝
        - 큐함수를 이용해서 다음 큐함수를 업데이트 하면, 뒤의 액션에 의해 앞의 액션이 영향을 받아 갇히는 현상이 발생할 수 있다
        - 따라서, 정책은 explore + 그리디를 조합해서 쓰는건 똑같은데
        - 큐함수를 업데이트할 때, 다음 큐함수로부터 업데이트 하지 않고, 다음 state에서 얻어지는 max q(s, a)만을 사용해서 업데이트 하자
        - 다음 스테이트 s'에서 최고의 큐를 반환해주는 a'을 그리디하게 선택해서 업데이트를 먼저 하고
        - 실제 이동은 입실론 탐험이 반영된 택한 경로에 의해 이동한다
        - 맥스를 선택하기 떄문에, 정책평가가 이미 가치함수에 반영된다
        - 정책평가 없이 반복적인 가치함수 업데이트!

    - reinforce : 정책을 gt에 의해 선택한다
        - 현재 정책 = 현재 네트워크
        - 정책 업데이트 = 네트워크 업데이트(by gt)
        - 입실론을 쓰지 않는다 : 모든 행동이 확률적으로 나오기 때문에, 확률뷴포에 따르는 선택을 한다

    - deepSarsa
        - 살사 알고리즘 : 큐함수 업데이트 + 정책 업데이트
        - 현재 큐함수 = 네트워크가 근사하는 함수
        - 큐함수 업데이트 = 네트워크 업데이트
        - td : 다음 행동을 미리 그리디하게 선택하고 업데이트

    - dqn
        - q러닝 알고리즘 : 큐함수 업데이트 + 정책 업데이트
        - 현재 큐함수 = 네트워크가 근사하는 함수
        - 큐함수 업데이트 = 네트워크 업데이트
        - td : sars'을 리플레이 메모리에서 랜덤하게 추출, 다음 q를 maxq 선택 후 업데이트
        - 타겟 네트워크와 업데이트 네트워크를 분리한다

    - a2c = reinforce + deepsarsa
        - value/ q : 네트워크
        - action : 네트워크
        - 큐 신경망 업데이트 = 다음 큐함수 - 밸류(td) 제곱을 최소화하는 방향으로 업데이트
        - 정책 신경망 업데이트( : 큐함수 업데이트를 가져다가 쓴다
        - 입실론을 쓰지 않는다 : 모든 행동이 확률적으로 나오기 때문에, 확률뷴포에 따르는 선택을 한다

    - a3c = reinforce + dqn
        - 여러 에이전트가 동시에 작동해서 리플레이 메모리 문제를 해결하고 글로벌 네트워크를 업데이트 한다
        - multi-step으로 만들어진 q를 쓴다
        - q에 엔트로피 수식을 추가해서 exploration 경향성을 추가한다
        
                
## 3년동안 생각한 헛소리 모음 정리중
1. 공간을 스프링과 노드로 구현 노드 : 좌표 에지 : 스프링
[김성주] [오전 12:32] 그래프 = 분자구조 로 보고
[김성주] [오전 12:32] 핵심은 정보를 벡터 대신 2개의 행렬로 생각하는 것임
[김성주] [오전 12:32] 2개의 행렬은 그래프를 표현한다
[김성주] [오전 12:33] 하나는 연결정보를 나타내는 행렬인데 중요한건 이게 연결이 됨 안됨 보다는 얼마나 가까운지 먼지에 따라 연결정도를 수로 나타내는게 좋으니까
[김성주] [오전 12:33] 이 매트릭스를 네트워크로 학습시키자
[김성주] [오전 12:33] 또 하나는 그 노드 하나에 담긴 정보인데
[김성주] [오전 12:33] 이 정보를 학습하는 것은 이미 완성되어 있어 보이니까 이대로 사용
[김성주] [오전 12:33] 중요한건 외부로 노출되는 부분의 정보를 담고 있는가?인거 같은데
[김성주] [오전 12:34] 하나의 노드가 가질 수 있는 최대 연결수를 정하고, 내부로 연결되어 있는 노드를 제외한 나머지 엣지를 가상으로 설정해서 노출된 엣지의 정보를 저장
[김성주] [오전 12:40] 노드의 연결성 메트랙스를 2개로 하자
[김성주] [오전 12:40] 1. 거리
[김성주] [오전 12:40] 2. 방향


2. 임베딩
    1. 임베딩1
        - 사전순으로 나열된 단어들이 있다. 이를 사전공간이라 한다
        - 이제 사전공간 >> 임베딩공간(개념공간임)으로 매핑을 하고싶다
        - 이제 이 임베딩공간에서 매핑일 일어나는 네트워크를 만들것이다.
        - 그러나 현재 순서가 반대로 되어있다 즉,
        - 임베딩공간은 정의했으나, 임베딩벡터가 존재하지 않고
        - 반대로 그 임베당공간상의 매핑결과는 인간이 생성한 수많은 문장에 의해 이미 결과로 얻어져 있는 상태가 된다
        - 따라서 다음과 같은 방식을 제안한다
        - 첫번째 임베딩벡터를 임의로 설정하고 >> 다음 임베딩벡터가 무엇인지 네트워크로 예측하고 >> 트레이닝셋에 맞도록 학습
        - 이렇게 하면 절대적인 임베딩벡터의 좌표는 얻을 수 없으나, 두 임베딩벡터의 상대적인 차이를 알게되고
        - 이러한 일을 모든 단어에 반복시행하면, 결국 모든 임베딩 벡터로 조합된 하나의 개념공간이 형성된다
        - 정리 : 임베딩단어 몇개 >> 타겟은 결정됨 >> 사전공간의 모든 벡터에 대해 타겟이 가능한지 확률추측
        - 소프트맥스로 최고확률을 가지는 단어를 결정 >> 답이 타겟과 다르면 같아지도록 트레이닝

    2. 임베딩2
        - 하나의 임베딩 벡터를 정하고, 이 임베딩벡터의 다음 매핑벡터를 다른 모든 사전벡터에 대해 비교하는게 아니라
        - k개의 제한된 사전벡터를 분포로 잡고 1개는 1, 나머지 벡터는 랜덤으로 불러서 0으로 매핑하는 네트워크를 만들자
        - 이 네트워크를 훈련시키면, 확률적으로 k개는 0일 확률이 크기 떄문에, 설마 0이 아니었다 해도 다음 훈련으로 상쇄되니까
        - 결국은 원하는 개념공간을 얻는다
        - 정리2 : 하나의 임베딩단어 >> 참인 타겟1개와 네거티브인 타겟k개를 결정 >> 즉, 사전공간의 k + 1개의 벡터에 대해
        - 참 거짓 유무를 최대한 맞추도록 소프트맥스로 트레이닝
        - 이제 생성한 개념공간을 정의해줄 차원축을 정의한다
        - 개념공간의 차원축 뽑아내기 : 개념공간을 형성했을 때, 각 특정 차원이 극단적인 값을 가지고 나머지는 아닌 벡터를 뽑으면 된다
        - 이미지만으로는 비어있는 감각에 대한 정보가 없어서 벡터가 불완선하고
        - 언어만으로는 비어있는 이미지에 대한 정보가 없어서 벡터가 불완전하고
        - 두 벡터공간을 하나로 만들 수 있다면 속성벡터가 완성되는데
        - 두 벡터공간을 하나로 하지 말고 연결만 하는게 지금의 방법인가?
        - 두 이미지간의 일치성에 의한 벡터소환을 어떻게 하는가? >> 샴 네트워크의 원샷러닝을 활용하자!
        - vae에서 가우시안이 아니라 가우시안의 합으로 근사
    
    3. 임베딩3
        - n차원 가우시안 분포가 있다고 하자
        - 가우시안 분포상 최대 확률을 가지는 가각의 집합셋ㅅ이 있다
        - 이 하나의 집합데이터를 벡터로 가정한다
        - 최대 확률만 가지는 각각의 랜덤변수 >> 하나의 벡터 : 임베딩 과정
        - 무엇을 임베딩하는가? : 임베딩한 대상끼리의 일련의 매핑경로가 규칙적으로 존재하는 대상
        - 이미지를 생성하는 벡터는 임베딩 대상이 되는가? : no!
        - 언어를 만드는 단어는 임베딩이 되는가? yes!
        - 소리를 생성하는 벡터는 임베딩 대상이 되는가? : no!
        - 음악을 만드는 소리는 임베딩이 된다! : yes!
        - 임베딩 대상이 되는 벡터란, 그 벡터의 연속적인 매핑에 대한 충분한 정보가 있어서, 한 벡터가 다른 벡터의 위치를 정할 수 있는 경우다
        - 객체를 [인식]하는 일 : 임베딩이 의미가 없다
        - 객체끼리의 [관계] : 임베딩이 필요하다
        - 상태는 하나의 현실인데, 이 상태를 표현하기 위해 순서가 있는 단어를 쓰는데
        - 단어는 각 객체와, 객체의 상태를 표현하기 떄문에, 연관관계가 있고, 임베딩 대상임
        - 하나의 이미지는, 생성해 내는데 서로간의 관계가 있는가?
        - 이미지를 생성하는 각 속성끼리의 연관관계는 : 객체간의 구별 + 3d 인식 + 각 객체의 특징

3. 대체 액션이 뭐지?
    - 만약 슈퍼마리오 화면을 생성하고 마리오의 움직임을 만들어 내는 속성벡터가 존재한다고 하자
    - 이 속성벡터가 5개라고 하자
    - 어떤 벡터는 마리오의 움직임 / 어떤 벡터는 버섯의 움직임을 만들어 낸다고 하자
    - 두 벡터는 서로 견제할 수밖에 없는 차원속성이 있다
    - 이 벡터집합이 목표하는 어떤 상태벡터가 있다고 하자
    - 이제 서로의 벡터가 서로의 벡터를 제한하는데, 이를 직관적으로 생각하면
    - 두 벡터가 매핑 도중에 특정 관계(각 + 방향)을 이루면 안되도록 제한하는 차원축의 집합들이 있다는 뜻이다
    - 모든 차원축이 수직관계임을 감안하면, 각은 항상 90이기 떄문에, 연관된 것은 차원축 집합의 각각의 길이 뿐이다

8. 하나의 매핑네트워크는 하나의 액션이다
    - 매핑네트워크는 [개념]을 다루는 네트워크다
    - 5개의 액션 네트워크가 학습되고, 후반 레이어로 갈수록 하나의 액션네트워크가 컨트롤할 수 있는 노드(차원의 수)는 많아진다
    - vs 후반으로 갈수록 액션네트워크들이 제한조건을 찾아내서 신경쓰지 않게되는 불변성노드들이 많아진다, 즉 다루어야 하는 신경쓸 노드의 수가 적어진다
    - ai의 목표 : 문제가 주어진다 >> 해결한다
    - 문제란 : 환경 + 제한조건 + 목표
    - 환경이란? : 제한조건과 연괸된 정보가 아닌 벡터의 차원정보들을 모두 환경정보라고 하자
    - 제한조건이란? : 벡터의 연속적인 매핑경로가 생성될 때, 특정 차원의 벡터값은 변할 수 없다
    - >> 벡터의 매핑경로는 특정 차원에서 한 평면 위에서만 움직인다
    - >> 제한조건이 늘어나면 벡터의 선택가능한 매핑경로는 줄어든다
    - 목표란? : 제한조건을 만족하면서 벡터 매핑경로의 도착점이 목표벡터에 도달
    - >> 벡터의 목표에 관련된 특정 차원의 정보가 목표값에 도달하는 것을 말한다

9. rnn
    - 벡터의 매핑경로 학습
    - 환경정보 : 현재 벡터의 정보 + 매핑경로상의 순서 및 위치
    - 벡터의 제한조건 : 직접적으로 알기 어려운 경우, 올바른 매핑결과를 트레이닝셋으로 제공
    - 제한조건이 숨어있어, 직접적으로 네트워크에 제시하기 어려운 경우에 사용한다
    - 숨어있는 복잡한 제한조건을 이미 완벽하게 만족하는 무수히 많은 매핑경로를 트레이닝셋으로 준다

10. reinforcement learning
    - 환경정보 : 매핑 경로상의 순서 및 위치정보가 없다
    - 제한조건 : 컨트롤할 수 있는 제한조건이 정해진 경우, 매핑결과를 찾아낼 수 있다.
    - 벡터 매핑경로의 제한조건이 매우 단순한 상황이라면(숨은 제한조건이 거의 없다면)
    - 드러난 제한조건과 관련된 차원축의 수도 적다 >> 컨트롤할 차원축의 수가 줄어든다
    - 단순한 액션 몇가지로 연관된 모든 차원축의 컨트롤이 가능해진다
    - (이렇지 않으면 모든 차원축의 정보를 컨트롤하면서 매핑경로를 찾아야 하는게 그것은 불가능하다)
    - 결과적으로 >> 매핑경로가 단순해진다
    - 제한조건과 관계된 차원정보[만] 수정해서 매핑경로를 찾아나가는 것이 강화학습이다
    - 앞으로 가는 키 : 벡터를 끊임없이 새롭게 매핑한다(랜덤성이 존재한다)
    - 점프 : 벡터의 여러 정보가 바뀌지만, 그 중에서 제한조건에 영향을 주는 차원정보가 있다면
    - 점프키를 활용해서 그것을 벗어나도록 훈련된다
    - 두개의 액션을 이용해서 벡터의 매핑경로가 제한조건 내에서 목표벡터에 도달하도록 훈련

    - 액션이란: 벡터가 가지고 있는 (제한조건과 연관된) 차원축의 정보들을 임의로 수정할 수 있는 매핑 네트워크
    - 즉, 액션은 =  주어진 벡터를 매핑하는 [네트워크]다
    - 생성된 액션네트워크를 활용해서(액션 = 벡터의 일부정보를 수정하는 매핑)
    - 제한조건과 연관된 정보만들 매핑하면서
    - 목표벡터에 도달하는 경로를 찾아나간다
    - 에이전트 : 매핑해야 하는 벡터공간의 subspace에 속한 하나의 네트워크로서
    - 주어진 벡터를 이 에이전트 네트워크로 연산하면 조건을 만족하면서 매핑시킬 수 있다
    - 생각은 작은 생각의 적당한 합이다 : 생각은 벡터의 매핑경로인데
    - 이 말은 네트워크의 합이 생각이 될 수 있다는 뜻이다
    - 액션네트워크가 가지는 특성1 : 액션 네트워크는 개념 네트워크에서 연결될 차원축을 [선택한다]
    - 액션네트워크 특성2 : 선택된 차원축의 값을 자유롭게 변형시킬 수 있다
    - 특성3 : 선택되는 차원축은 제한조건에 연관된 차원축이다 / 불변차원축은 건드리지 않는다
    - 액션 네트워크는 개념공간의 벡터를 매핑하는데 사용된다 >> 즉 개념 자체를 다룬다
    - 개념공간에서 다음 개념공간으로 매핑된 벡터를 >> 실제 데이터 벡터로 매핑한다
    - 연속적인 매핑의 결론이 최종적인 매핑이 되엇다고 가정하자
    - 그러나 주어지는 모든 상태에 모조리 동일한 네트워크를 써서 매핑된 결론이 완벽한 참일 수  없다
    - 왜냐하면 상황에 따라 다른 매핑이 필요하고,
    - 그 매핑이란게 매우 예상치 못한 매핑이 필요했다면, 지금의 네트워크로는 그러한 매핑을 할 수 없기 때문이다
    - 이게 rnn이다
    - 여기서 다중액션의 필요성이 등장한다. 즉 액션이란, 이러한 매핑[네트워크]고
    - 액션이 5개라는 뜻은 상황에 따라 선택할 수 있는 매핑 네트워크가 5개라는 뜻이다
    - 이제 작은 액션이 모여 큰 액션이 될 수 있다는 아이디어 아래
    - 처음에는 300차원 중에서 10개의 차원축만을 컨트롤 하는 액션네트워크가 5개 있다고 한다면
    - 액션을 활용할 수록, 300차원중에 200개의 차원축을 컨트롤 하는 액션 네트워크가 5개 존재해야 한다
    - 최초의 액션 네트워크가 5개의 노드를 선택해서 파라미터를 설정하고 컨트롤했다면
    - 다음다음다음 액션 네트워크는 이제 각 액션네트워크끼리 종합하는 세로운 네트워크를 형성하고
    - 그 네트워크가 주어진 차원정보를 한꺼번에 여러개 컨트롤할 수 있어야 하고
    - 각 차원축을 조절하는 액션네트워크는 당연히 겹칠 수 있다
    - 이것이 바로 종합적인 생각의 정의다
    - 그러나 액션에 따라 어떤 차원을 증가하기도, 감소하기도 하기 떄문에, 두 액션의 컨트롤차원을 하나로 더하는데 문제가 있다 : 미해결!
    - 불변차원을 늘리고 컨트롤할필요가 없는 차원을 제외한는 액션생성방법으로 바꾸자! : 해결!
    - 실제로 언어와 이미지개념벡터로 다음 벡터를 매핑하는 방법을 생각해 보면 다음과 같다
    - 언어공간의 매핑정보 : 액션이 아니라 rnn으로 학습된 네트워크를 이미 가지고 있다 : 수많은 숨은 조건을 이미 간접학습한 네트워크
    - 이미지공간의 매핑정보 : rnn으로 이미지의 연속성이나 물리적 법칙의 수많은 숨은조건들을 이미 간접학습한 네트워크
    - 둘 다 지금은 rnn때문에 단일 매핑 액션 네트워크를 가지고 있다
    - 일단 이것부터 바꾸자
    - 트레이닝셋을 가지고 rnn을 써서 액션을 생성할 때,
    - 벡터의 노드를 제한해서 던져보자 >> 제한된 정보를 다음 벡터로 최대한 매핑하기 위해 특정한 정보를 다루는 액션이 생겨날 수 있다
    - 서로 다르게 벡터의 개념차원정보를 제한해서 >> 각각의 개념정보에 대해 다루는 액션벡터를 다르게 생성한다
    - 이제 생성된 5개의 액션네트워크를 가지고 전체 트레이닝셋을 강화학습하면
    -  액션의 배열정보만 알게될 뿐, 액션 네트워크 자체는 그대로다
    - 강화학습으로 얻어진 각 매핑 시퀀스를
    - rnn에 넣으면 인해 각 매핑마다 코스트함수가 생기는데, 이 코스트함수의 총합을 감소시켜서 이용해서 액션을 1회 업데이트
    - 반복시행한다
    - 노드에 관한 생각을 적어보면 다음과 같다.
    - 우선 어떤 [단일]액션이 트레이닝된다는 뜻은, 그 액션 네트워크가 다루는 자체 내부적으로는 매핑할때 노드끼리의 내부충돌이 없다는 뜻이다
    - 그러나, 두 액션을 마음대로 결합할 수는 없다. 왜나하면
    - 두 액션 네트워크가 만약 공유되는 노드(차원정보)를 가지고 있다면 두 액션은 독립적으로 작용할 수 없기 때문이다.
    - 왜냐하면, 어떤 액션네트워크가 노드1을 감소시켜 매핑하는데
    - 어떤 액션네트워크가 노드1을 증가시켜 매핑한다면
    - 두 네트워크는 공존할 수 없기 때문이다
    - 따라서 두 네트워크가 독립적일 수 있도록, 트레이닝된 액션네트워크의 노드와 파라미터는 조합될 수 없다.
    - 액션끼리 더하는게 아니라
    - 액션으로 인해 판단된 관계없는 노드를 제거하고 새로운 노드를 추가하는 방법을 쓰자
    - 하나의 문제를 해결하기 위해 필요한 숨은 제한조건을 모두 찾아 고정하고
    - 해결에 필요한 차원정보를 최소한으로 제한해서 결정한 다음
    - 그 차원정보를 해결하는 n개의 액션을 생성해서
    - 매핑으로 문제를 해결하는 방식으로 해보자
    - 다음과 같은 방법을 제안한다
    - 5개의 초기액션으로 모든 매핑을 성공적으로 마친 경우, 코스트 함수가 제시되고 액센네트워크는 업데이트된다
    - 이 때, 각 액션으로 인해 벡터가 매핑되는데, 벡터의 매핑결과, 값이 거의 변화하지 않는 벡터의 차원정보가 있을 것이다
    - 이 불변차원의 정보를 어케 찾는가 하면 >> 어텐션을 쓴다
    - 어텐션 파라미터를 써서 주목되지 않는 노드를 알아내고 확률적으로 매번 다음 샘플링때 선택될 확률이 감소하도록 만들자
    - 어텐션 파라미터가 주목한 노드는 다음 샘플링때 선택될 확률이 증가하도록 한다
    - 불변차원정보는 각 액션마다 다르게 선택하고 버린다
    - 왜냐하면 어떤 액션에서는 다른 액션에서 불변으로 생각하는 정보가 불변이 아닐 가능성이 있기 때문이다
    - 이렇게 하면 액션의 수는 일정하게 유지한 채로, 각각의 독립성을 유지하면서, 벡터를 최선으로 매핑하는 액션을 얻게된다
    - 주의사항 : 한 액션네트워크에서 제외된 노드는 다른 액션네트워크 노드가 될 수 있다!
    - 액션의 수가 늘어나면 강화학습이 어려워 지고, 액션의 수가 줄어들면 매핑의 완전성이 감소한다, 둘의 적당한 중간지점을 취해주는 것이 포인트다
    - 종합개념벡터의 매핑
    - 각 차원공간에 존재하는 5개의 학습된 액션이 존재한다고 가정하자
    - 현재 주어진 액션은 15개다
    - 만약 3공간의 벡터를 하나로 합성한 경우, 액션 역시 합성해야 한다
    - 3 공간의 차원축을 연결했기 때문에
    - 각 액션네트워크 역시 연결할 수 있다?
    - rnn이란 액션추출기다
    - rnn은 각 매핑마다 하나의 네트워크를 써서 다음 벡터를 매핑한다
    - 따라서 rnn은 각 매핑마다 하나의 액션을 쓰는데, 매핑에 100번이라면 네트워크가 100번 존재한다
    - 그런데 rnn의 네트워크 구조를 잘 생각해보면, 모든 매핑에서 사용된 w와 b가 모두 동일하다
    - 이 말의 의미는, 결국 rnn네트워크의 구조는 단 하나의 액션을 사용하는 강화학습이라는 뜻이다
    - rnn이 강화학습과 다른점은, rnn은 결과를 미리 제시해서 하나의 액션 자체를 정의하고 학습하지만
    - 강화학습은 결과가 미리 제시되지 않기 뗴문에, 단순한 액션을 미리 정의하고 매핑을 찾아간다는 것이다
    - 따라서 모델을 생성할 때 생각할 단순한 조건은 다음과 같다
    - 결과가 미리 제시된 경우(트레이닝셋이 있는 경우) : 적당한 하나의 액션을 추출할 수 있다
    - 결과를 미리 알 수 없는 경우(트레이닝 셋이 없다) : 적당한 몇개의 액션을 미리 제시하고 시작해야 한다
    - rnn의 모델이란 결국 다음과 같다
    - 하나의 개념공간 내의 벡터끼리 숨은 조건을 고려해서 만들어진 트레이닝셋의 매핑경로를 보고
    - 그 모든 매핑경로를 포함할 수 있는 하나의 액션을 트레이닝으로 생성한 것이다
    - 이 액션을 여러개를 생성하고, 각 매핑마다 적당한 액션을 선택할 수 있게 하는것이 내가 생각하는 아이디어다
    - 또, rnn은 주어진 벡터의 모든 차원을 전부 다 조절하는 액션벡터를 1개 생성했다
    - 내가 생각하는 방법은, 주어진 벡터의 일부 sub차원만 조절하는 다중액션벡터를 10개 생성하고
    - 각 매핑마다 맞는 액션을 취사선택할 수 있도록 변형하는 것이다
    - rnn의 단일액션네트워크가 업데이트되는 방법을 응용해보자
    - 각 매핑마다 발생하는 코스트함수를 모조리 더해서, 그것을 하나의 코스트함수로 보고 트레이닝한다
    - 마찬가지 방법을 응용하면 다음과 같은 방법이 제안된다
    - 랜덤한 다중액션네트워크를 일단 생성한다
    - 조절할 노드와 파라미터 모드 랜덤이다
    - 이제 이 네트워크가 알맞은 매핑을 할 수 있도록 인풋을 주고 아웃풋을 결정해 주면 네트워크는 훈련된다
    - 인풋은 매핑을 원하는 벡터가 된다
    - 아웃풋은 매핑결과다
    - 매핑결결과가 옳은지 어떻게 판단할까?
    - 방법1 : rnn과 같은 방법으로 한다. 즉 트레이닝셋을 제시하고,
    - 각 매핑에서 액션을 선택해서 나온 코스트를 모두 더한 후, 액션네트워크를 업데이트한다
    - 방법2 : 트레이닝셋이 없는 경우 감별자가 트레이닝셋 대신에 옳은지 그른지 판단하면 좋겠다
    - 감별자는 무엇을 판단하는가?
    - 매핑된 벡터가 내가 원하는 매핑벡터인지 판단하면 되는데
    - 현재 매핑이 내가 원하는 매핑인지 알 방법이 없다. 그러나!
    - 이런 매핑이 연속적으로 일어난 결과매핑벡터가 내가 원하는 게임의 목표라면, 리워드를 주기 떄문에
    - 바로 이 리워드 = 코스트함수로 해서, 코스트함수를 미분하는 방식으로 각 매핑에서 사용되었던 액센네트워크의 파라미터를 업데이트한다
    - attention을 이용해서, 각 매핑에 주목되었던 모든 노드를 기록해 두어야 한다
    - 이 기록을 활용해서 어떤 노드가 주로 활용되는지 기록한 후에
    - 다음 액션에 전승되도록 한 후에 액션네트워크가 컨트롤하는 노드의 수를 증가시킨다
    - 지금 잘 하고 있는건지 판단하는 감별자를 만들고
    - 이 감별자는 다른사람의 플레이나 셀프플레이를 보고 만들어진 감별자로
    - 결과가 안나와도 중간중간에 각 개념요소별로 잘하는지아닌지 중간리워드를많이 제시한다
    - 코스트함수를 음수로 만드는것이 매핑네트워크의 목표가 되도록 가가오하학습
    - 결국 강화학습의 리워느 = gan의 감별자 코스트함수의 단순화된 형태

11. 정리하면 다음과 같다
    - 우선 스타크래프트 화면이 존재한다
    - 주어지는 제한조건이 매우 단순하다
    - 제한조건1 : 목표 : 게임에서 승리한다
    - 제한조건2 : 패배하지 않는다
    - 그러나, 실제로 플레이를 시작하면, 이보다 훨씬 더 많은 [숨은] 제한조건이 존재한다
    - 그러나 제한조건이 무엇인지 아직 모르는 상태이기 때문에
    - 벡터의 매핑경로가 무한대에 가깝고, 따라서 결과벡터에 도달하기 어렵게 된다
    - 숨은 제한조건을 모두 고려하는 매핑경로 탐색이란 있을 수 없다
    
    - 매핑경로 제한방법1
    - 인간에 의헤 제한조건을 이미 고려해서 일어난 대강의 매핑경로를 rnn으로 학습한다
    - 숨은 제한조건을 간접적으로 학습
    
    - 매핑경로 제한방법2
    - 3가지 차원공간에서 온 종합벡터 : 벡터의 매핑경로에 많은 제한조건이 추가된다
    - 위의 2가지 방법 모두 매핑경로를 단순하게 만든다
    - 만약 강화학습을 먼저 선택한 경우를 고려한다면
    - (다중차원정보벡터로) 단순한 매핑경로 상태에서 액션을 사용해서 적당한 매핑경로를 뽑아낼 수 있다
    - 물론 이는 장기적으로 최적경로가 아니다
    - 이제 선택된 매핑경로를 방법1로 학습한다
    - 다시 방법2를 활용해서 매핑경로를 뽑는데 rnn으로 학습되어 샘플링된 매핑경로의 도움을 받자.
    - 도움을 받는 방법은 아래와 같다.
    - rnn이 제시하는 매핑경로는 최적은 아니지만 목표에 도달 가능한 매핑경로이기 떄문에
    - 전체적인 매핑경로는 비효율적이지만, 국소적인 매핑은 최적인 경우가 존재한다
    - 따라서 rnn이 제시하는 매핑경로 정보를 벡터화한 다음에
    - 이것의 [국소적인] 부분을 [attention]해서 도우미정보벡터로 변환하자
    - 이제 이 도우미정보벡터를 현재 강화학습의 벡터정보에 추가한다
    - 이제 이 벡터를 다음 벡터로 매핑하는 액션을 취한다
    - rnn이 제시하는 국소적인 정보가 추가되어 다음 매핑이 더 확실해진다
    - 만약 제4의 차원공간이 생성된다면, 이후로 일어나는 제5의 차원공간 학습에 4개의 차원공간 정보가 활용될 수 있다.
    - 차춴공간과매핑네트워크의 다양성으로 인해 생각은 계속 정교해지고 다양한 방법으로 생각하게 된다.
    - 차원공간을 늘리는데 순서가 필요하기 때문에, 사고를 훈련하는데 제시하는 순서가 중요하다

12. 기타
    - rnn 번역이 완벽하지 않는 이유는?:
    - 주어진 벡터가 고정된 자리에 존재하거나 거의 변화하지 않는 경우에
    - 처벌함수를 생성해서 벡터가 끊임없이 새로운 자리로 매핑될 수 있도록
    - 설계한다 >> 만약 마우스를 클릭해도 주어진 환경이미지의
    - 벡터에 거의 변화가 없는 경우인데 제한조건에 걸리지 않는다면?
    - 이 차원축은 환경정보일 가능성이 크다
    - 항원 항체 반응을 본따서 생각의 구조를 만들자
    - 수용체의 형태는 미분기하에 의해 변형될 수 있다
    - 변화를 관측하면 이 변화를 개념으로 저장한다